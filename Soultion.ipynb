{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8556550a",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The first steps to get started are:\n",
    "1. Get the setup command\n",
    "2. Execute it in the cell below\n",
    "\n",
    "### >> https://hub.crunchdao.com/competitions/structural-break/submit/notebook\n",
    "\n",
    "![Reveal token](https://raw.githubusercontent.com/crunchdao/competitions/refs/heads/master/documentation/animations/reveal-token.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0fd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crunch-cli in c:\\users\\alon\\anaconda3\\lib\\site-packages (7.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (8.1.7)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (15.0.1)\n",
      "Requirement already satisfied: dataclasses_json in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (0.6.7)\n",
      "Requirement already satisfied: importlib_metadata in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (7.0.1)\n",
      "Requirement already satisfied: inquirer in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (3.4.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (1.4.2)\n",
      "Requirement already satisfied: libcst in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (1.8.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (3.3)\n",
      "Requirement already satisfied: packaging>=24.2 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (25.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (2.3.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (21.0.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: requirements-parser>=0.11.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (0.13.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (1.16.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alon\\anaconda3\\lib\\site-packages (from crunch-cli) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\alon\\anaconda3\\lib\\site-packages (from click->crunch-cli) (0.4.6)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from coloredlogs->crunch-cli) (10.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from dataclasses_json->crunch-cli) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from dataclasses_json->crunch-cli) (0.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from importlib_metadata->crunch-cli) (3.17.0)\n",
      "Requirement already satisfied: blessed>=1.19.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from inquirer->crunch-cli) (1.21.0)\n",
      "Requirement already satisfied: editor>=1.6.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from inquirer->crunch-cli) (1.6.6)\n",
      "Requirement already satisfied: readchar>=4.2.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from inquirer->crunch-cli) (4.2.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas->crunch-cli) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas->crunch-cli) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas->crunch-cli) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas->crunch-cli) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from requests->crunch-cli) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from requests->crunch-cli) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from requests->crunch-cli) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from requests->crunch-cli) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from scikit-learn->crunch-cli) (3.5.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (0.2.5)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from blessed>=1.19.0->inquirer->crunch-cli) (1.3.0)\n",
      "Requirement already satisfied: runs in c:\\users\\alon\\anaconda3\\lib\\site-packages (from editor>=1.6.0->inquirer->crunch-cli) (1.2.2)\n",
      "Requirement already satisfied: xmod in c:\\users\\alon\\anaconda3\\lib\\site-packages (from editor>=1.6.0->inquirer->crunch-cli) (1.8.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->crunch-cli) (3.5.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->crunch-cli) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json->crunch-cli) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses_json->crunch-cli) (4.11.0)\n",
      "Requirement already satisfied: ansicon in c:\\users\\alon\\anaconda3\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.19.0->inquirer->crunch-cli) (1.89.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "crunch-cli, version 7.5.0\n",
      "\n",
      "---\n",
      "Your token seems to have expired or is invalid.\n",
      "\n",
      "Please follow this link to copy and paste your new setup command:\n",
      "https://hub.crunchdao.com/competitions/structural-break/submit\n",
      "\n",
      "If you think that is an error, please contact an administrator.\n"
     ]
    }
   ],
   "source": [
    "# Install the Crunch CLI\n",
    "%pip install --upgrade crunch-cli\n",
    "\n",
    "# Setup your local environment\n",
    "!crunch setup --notebook structural-break hello --token aaabbbcccddeeff00112233445566778899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1112a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model\n",
    "## Setup\n",
    "import os\n",
    "import typing\n",
    "\n",
    "# Import your dependencies\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a047a007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n",
      "\n",
      "cli version: 7.5.0\n",
      "available ram: 63.78 gb\n",
      "available cpu: 32 core\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "\n",
    "# Load the Crunch Toolings\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04145e",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "\n",
    "The dataset consists of univariate time series, each containing ~2,000-5,000 values with a designated boundary point. For each time series, you need to determine whether a structural break occurred at this boundary point.\n",
    "\n",
    "The data was downloaded when you setup your local environment and is now available in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44d1a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
      "data\\X_train.parquet: already exists, file length match\n",
      "data\\X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
      "data\\X_test.reduced.parquet: already exists, file length match\n",
      "data\\y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
      "data\\y_train.parquet: already exists, file length match\n",
      "data\\y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
      "data\\y_test.reduced.parquet: already exists, file length match\n"
     ]
    }
   ],
   "source": [
    "# Load the data simply\n",
    "X_train, y_train, X_test = crunch.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb843c",
   "metadata": {},
   "source": [
    "### Understanding `X_train`\n",
    "\n",
    "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
    "\n",
    "**Index Levels:**\n",
    "- `id`: Identifies the unique time series\n",
    "- `time`: The timestep within each time series\n",
    "\n",
    "**Columns:**\n",
    "- `value`: The actual time series value at each timestep\n",
    "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0d745f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.005564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013164</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.009979</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">10000</th>\n",
       "      <th>2134</th>\n",
       "      <td>0.001137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>0.003526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>0.000687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>0.001640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>0.001074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23715734 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               value  period\n",
       "id    time                  \n",
       "0     0    -0.005564       0\n",
       "      1     0.003705       0\n",
       "      2     0.013164       0\n",
       "      3     0.007151       0\n",
       "      4    -0.009979       0\n",
       "...              ...     ...\n",
       "10000 2134  0.001137       1\n",
       "      2135  0.003526       1\n",
       "      2136  0.000687       1\n",
       "      2137  0.001640       1\n",
       "      2138  0.001074       1\n",
       "\n",
       "[23715734 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd8f92",
   "metadata": {},
   "source": [
    "### Understanding `y_train`\n",
    "\n",
    "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
    "\n",
    "**Index:**\n",
    "- `id`: the ID of the dataset\n",
    "\n",
    "**Value:**\n",
    "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c979da59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0        False\n",
       "1        False\n",
       "2         True\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "9996     False\n",
       "9997     False\n",
       "9998     False\n",
       "9999     False\n",
       "10000     True\n",
       "Name: structural_breakpoint, Length: 10001, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d9cd6",
   "metadata": {},
   "source": [
    "### Understanding `X_test`\n",
    "\n",
    "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
    "\n",
    "\n",
    "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b950fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 101\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datasets:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3edae28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">10001</th>\n",
       "      <th>0</th>\n",
       "      <td>0.010753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.031915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.011111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>-0.013937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2775</th>\n",
       "      <td>-0.015649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2776</th>\n",
       "      <td>-0.009744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>0.025375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>-0.001515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2779 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               value  period\n",
       "id    time                  \n",
       "10001 0     0.010753       0\n",
       "      1    -0.031915       0\n",
       "      2    -0.010989       0\n",
       "      3    -0.011111       0\n",
       "      4     0.011236       0\n",
       "...              ...     ...\n",
       "      2774 -0.013937       1\n",
       "      2775 -0.015649       1\n",
       "      2776 -0.009744       1\n",
       "      2777  0.025375       1\n",
       "      2778 -0.001515       1\n",
       "\n",
       "[2779 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8e4ef",
   "metadata": {},
   "source": [
    "# model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f102d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\alon\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\alon\\anaconda3\\lib\\site-packages (0.14.2)\n",
      "Requirement already satisfied: numba in c:\\users\\alon\\anaconda3\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: pywavelets in c:\\users\\alon\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\alon\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alon\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\alon\\anaconda3\\lib\\site-packages (from xgboost) (1.16.1)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from statsmodels) (2.3.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alon\\anaconda3\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\alon\\anaconda3\\lib\\site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost statsmodels numba pywavelets lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# If you want to use LightGBM (optional)\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "class FastDGPFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Optimized DGP feature extractor focusing on speed and essential features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Focus on fast, essential estimators only\n",
    "        self.fast_estimators = {\n",
    "            'gaussian': self._estimate_gaussian_fast,\n",
    "            'ar1': self._estimate_ar1_fast, \n",
    "            'volatility': self._estimate_volatility_fast,\n",
    "            'trend': self._estimate_trend_fast,\n",
    "            'spectral': self._estimate_spectral_fast,\n",
    "            'jumps': self._estimate_jumps_fast,\n",
    "            'autocorr': self._estimate_autocorr_fast,\n",
    "            'distribution': self._estimate_distribution_fast,\n",
    "            # New DGP estimators\n",
    "            'ma_process': self._estimate_ma_process_fast,\n",
    "            'ar_process': self._estimate_ar_process_fast,\n",
    "            'arma_process': self._estimate_arma_process_fast,\n",
    "            'garch_process': self._estimate_garch_process_fast,\n",
    "            'ornstein_uhlenbeck': self._estimate_ornstein_uhlenbeck_fast,\n",
    "            'geometric_brownian_motion': self._estimate_geometric_brownian_motion_fast,\n",
    "            'levy_stable': self._estimate_levy_stable_fast\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, X_train: pd.DataFrame, max_series: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fast feature extraction optimized for speed\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pd.DataFrame\n",
    "            MultiIndex DataFrame with levels ['id', 'time'] and columns ['value', 'period']\n",
    "        max_series : int, optional\n",
    "            Limit number of series to process (for testing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Features with columns for full, period0, period1, and differences\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"Starting optimized DGP feature extraction...\")\n",
    "        \n",
    "        # Get unique IDs\n",
    "        unique_ids = X_train.index.get_level_values('id').unique()\n",
    "        if max_series:\n",
    "            unique_ids = unique_ids[:max_series]\n",
    "        \n",
    "        #print(f\"Processing {len(unique_ids)} time series with {len(self.fast_estimators)} estimators\")\n",
    "        \n",
    "        # Pre-allocate results list\n",
    "        all_features = []\n",
    "        \n",
    "        # Batch processing for efficiency\n",
    "        for i, ts_id in enumerate(unique_ids):\n",
    "            if i % 50 == 0:  # Less frequent updates\n",
    "                #print(f\"Processed {i}/{len(unique_ids)} series\")\n",
    "            \n",
    "                try:\n",
    "                    # Extract data efficiently\n",
    "                    ts_data = X_train.loc[ts_id]\n",
    "                    \n",
    "                    # Vectorized operations\n",
    "                    values = ts_data['value'].values\n",
    "                    periods = ts_data['period'].values\n",
    "                    \n",
    "                    # Split series efficiently\n",
    "                    mask_p0 = periods == 0\n",
    "                    mask_p1 = periods == 1\n",
    "                    \n",
    "                    full_series = values\n",
    "                    period0_series = values[mask_p0] if mask_p0.any() else np.array([])\n",
    "                    period1_series = values[mask_p1] if mask_p1.any() else np.array([])\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = self._extract_features_fast(ts_id, full_series, period0_series, period1_series)\n",
    "                    all_features.append(features)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {ts_id}: {str(e)}\")\n",
    "                    all_features.append({'id': ts_id, 'error': str(e)})\n",
    "        \n",
    "        # Convert to DataFrame efficiently\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        if 'id' in features_df.columns:\n",
    "            features_df.set_index('id', inplace=True)\n",
    "        \n",
    "        #print(f\"Feature extraction completed in optimized mode. Shape: {features_df.shape}\")\n",
    "        return features_df\n",
    "    \n",
    "    def _extract_features_fast(self, ts_id: str, full_series: np.ndarray, \n",
    "                              period0_series: np.ndarray, period1_series: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Fast feature extraction using vectorized operations\n",
    "        \"\"\"\n",
    "        \n",
    "        features = {'id': ts_id}\n",
    "        \n",
    "        # Minimum length check\n",
    "        min_len = 5\n",
    "        series_dict = {\n",
    "            'full': full_series if len(full_series) >= min_len else np.array([]),\n",
    "            'period0': period0_series if len(period0_series) >= min_len else np.array([]),\n",
    "            'period1': period1_series if len(period1_series) >= min_len else np.array([])\n",
    "        }\n",
    "        \n",
    "        # Storage for differences\n",
    "        p0_params = {}\n",
    "        p1_params = {}\n",
    "        \n",
    "        # Apply fast estimators\n",
    "        for prefix, series in series_dict.items():\n",
    "            if len(series) == 0:\n",
    "                continue\n",
    "                \n",
    "            for est_name, est_func in self.fast_estimators.items():\n",
    "                try:\n",
    "                    params = est_func(series)\n",
    "                    \n",
    "                    # Flatten and store with prefix\n",
    "                    for param_name, param_value in params.items():\n",
    "                        if isinstance(param_value, (int, float, np.number)) and np.isfinite(param_value):\n",
    "                            col_name = f\"{prefix}_{est_name}_{param_name}\"\n",
    "                            features[col_name] = float(param_value)\n",
    "                    \n",
    "                    # Store for difference calculation\n",
    "                    if prefix == 'period0':\n",
    "                        p0_params[est_name] = params\n",
    "                    elif prefix == 'period1':\n",
    "                        p1_params[est_name] = params\n",
    "                        \n",
    "                except:\n",
    "                    continue  # Skip failed estimations silently for speed\n",
    "        \n",
    "        # Compute differences efficiently\n",
    "        if p0_params and p1_params:\n",
    "            diff_features = self._compute_differences_fast(p0_params, p1_params)\n",
    "            features.update(diff_features)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_differences_fast(self, p0_params: Dict, p1_params: Dict) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Fast difference computation\n",
    "        \"\"\"\n",
    "        diff_features = {}\n",
    "        \n",
    "        for est_name in set(p0_params.keys()) & set(p1_params.keys()):\n",
    "            p0 = p0_params[est_name]\n",
    "            p1 = p1_params[est_name]\n",
    "            \n",
    "            for param_name in set(p0.keys()) & set(p1.keys()):\n",
    "                try:\n",
    "                    val0 = p0[param_name]\n",
    "                    val1 = p1[param_name]\n",
    "                    \n",
    "                    if isinstance(val0, (int, float, np.number)) and isinstance(val1, (int, float, np.number)):\n",
    "                        if np.isfinite(val0) and np.isfinite(val1):\n",
    "                            # Absolute difference\n",
    "                            diff_features[f\"diff_{est_name}_{param_name}_abs\"] = float(val1 - val0)\n",
    "                            \n",
    "                            # Relative difference (safe division)\n",
    "                            if abs(val0) > 1e-8:\n",
    "                                diff_features[f\"diff_{est_name}_{param_name}_rel\"] = float((val1 - val0) / val0)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        return diff_features\n",
    "    \n",
    "    # ==================== FAST ESTIMATORS ====================\n",
    "    \n",
    "    def _estimate_gaussian_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast Gaussian parameter estimation\"\"\"\n",
    "        return {\n",
    "            'mean': np.mean(ts),\n",
    "            'std': np.std(ts, ddof=1),\n",
    "            'skew': self._skewness_fast(ts),\n",
    "            'kurt': self._kurtosis_fast(ts)\n",
    "        }\n",
    "    \n",
    "    def _estimate_ar1_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast AR(1) estimation using simple correlation\"\"\"\n",
    "        if len(ts) < 3:\n",
    "            return {'phi': 0.0, 'sigma': np.std(ts)}\n",
    "        \n",
    "        # Simple AR(1) using correlation\n",
    "        x_lag = ts[:-1]\n",
    "        x_curr = ts[1:]\n",
    "        \n",
    "        # Avoid correlation computation if no variance\n",
    "        if np.std(x_lag) < 1e-10 or np.std(x_curr) < 1e-10:\n",
    "            phi = 0.0\n",
    "        else:\n",
    "            phi = np.corrcoef(x_lag, x_curr)[0, 1]\n",
    "            if not np.isfinite(phi):\n",
    "                phi = 0.0\n",
    "        \n",
    "        # Residual variance\n",
    "        residuals = x_curr - phi * x_lag\n",
    "        sigma = np.std(residuals, ddof=1) if len(residuals) > 1 else np.std(ts)\n",
    "        \n",
    "        return {'phi': phi, 'sigma': sigma}\n",
    "    \n",
    "    def _estimate_volatility_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast volatility estimation\"\"\"\n",
    "        if len(ts) < 3:\n",
    "            return {'vol_mean': np.std(ts), 'vol_std': 0.0}\n",
    "        \n",
    "        # Rolling volatility (squared returns)\n",
    "        returns_sq = np.diff(ts) ** 2\n",
    "        \n",
    "        return {\n",
    "            'vol_mean': np.mean(returns_sq),\n",
    "            'vol_std': np.std(returns_sq, ddof=1),\n",
    "            'vol_persistence': np.corrcoef(returns_sq[:-1], returns_sq[1:])[0, 1] if len(returns_sq) > 2 else 0.0\n",
    "        }\n",
    "    \n",
    "    def _estimate_trend_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast trend estimation\"\"\"\n",
    "        n = len(ts)\n",
    "        if n < 3:\n",
    "            return {'trend': 0.0, 'trend_pval': 1.0}\n",
    "        \n",
    "        # Simple linear trend using least squares\n",
    "        x = np.arange(n)\n",
    "        \n",
    "        # Vectorized least squares\n",
    "        x_mean = np.mean(x)\n",
    "        y_mean = np.mean(ts)\n",
    "        \n",
    "        numerator = np.sum((x - x_mean) * (ts - y_mean))\n",
    "        denominator = np.sum((x - x_mean) ** 2)\n",
    "        \n",
    "        if denominator < 1e-10:\n",
    "            return {'trend': 0.0, 'trend_rsq': 0.0}\n",
    "        \n",
    "        slope = numerator / denominator\n",
    "        \n",
    "        # R-squared\n",
    "        y_pred = slope * (x - x_mean) + y_mean\n",
    "        ss_res = np.sum((ts - y_pred) ** 2)\n",
    "        ss_tot = np.sum((ts - y_mean) ** 2)\n",
    "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 1e-10 else 0.0\n",
    "        \n",
    "        return {'trend': slope, 'trend_rsq': r_squared}\n",
    "    \n",
    "    def _estimate_spectral_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast spectral analysis using FFT\"\"\"\n",
    "        n = len(ts)\n",
    "        if n < 8:\n",
    "            return {'dom_freq': 0.0, 'spectral_entropy': 0.0}\n",
    "        \n",
    "        # Detrend\n",
    "        ts_detrend = ts - np.linspace(ts[0], ts[-1], n)\n",
    "        \n",
    "        # FFT\n",
    "        fft_vals = np.fft.fft(ts_detrend)\n",
    "        power = np.abs(fft_vals[:n//2]) ** 2\n",
    "        \n",
    "        # Dominant frequency\n",
    "        dom_idx = np.argmax(power[1:]) + 1  # Skip DC component\n",
    "        dom_freq = dom_idx / n\n",
    "        \n",
    "        # Spectral entropy\n",
    "        power_norm = power / np.sum(power)\n",
    "        power_norm = power_norm[power_norm > 1e-10]  # Avoid log(0)\n",
    "        spectral_entropy = -np.sum(power_norm * np.log(power_norm))\n",
    "        \n",
    "        return {'dom_freq': dom_freq, 'spectral_entropy': spectral_entropy}\n",
    "    \n",
    "    def _estimate_jumps_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast jump detection\"\"\"\n",
    "        if len(ts) < 3:\n",
    "            return {'jump_count': 0, 'jump_intensity': 0.0}\n",
    "        \n",
    "        # Simple jump detection using threshold\n",
    "        returns = np.diff(ts)\n",
    "        threshold = 3 * np.std(returns, ddof=1)\n",
    "        \n",
    "        jumps = np.abs(returns) > threshold\n",
    "        jump_count = np.sum(jumps)\n",
    "        jump_intensity = np.sum(np.abs(returns[jumps])) if jump_count > 0 else 0.0\n",
    "        \n",
    "        return {'jump_count': float(jump_count), 'jump_intensity': jump_intensity}\n",
    "    \n",
    "    def _estimate_autocorr_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast autocorrelation estimation\"\"\"\n",
    "        n = len(ts)\n",
    "        if n < 4:\n",
    "            return {'acf_1': 0.0, 'acf_2': 0.0, 'acf_5': 0.0}\n",
    "        \n",
    "        # Center the series\n",
    "        ts_centered = ts - np.mean(ts)\n",
    "        \n",
    "        # Compute autocorrelations for lags 1, 2, 5\n",
    "        autocorrs = {}\n",
    "        for lag in [1, 2, min(5, n//2)]:\n",
    "            if lag < n:\n",
    "                corr = np.corrcoef(ts_centered[:-lag], ts_centered[lag:])[0, 1]\n",
    "                autocorrs[f'acf_{lag}'] = corr if np.isfinite(corr) else 0.0\n",
    "            else:\n",
    "                autocorrs[f'acf_{lag}'] = 0.0\n",
    "        \n",
    "        return autocorrs\n",
    "    \n",
    "    def _estimate_distribution_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast distributional properties\"\"\"\n",
    "        return {\n",
    "            'range': np.ptp(ts),  # max - min\n",
    "            'iqr': np.percentile(ts, 75) - np.percentile(ts, 25),\n",
    "            'q95_q5': np.percentile(ts, 95) - np.percentile(ts, 5),\n",
    "            'median_abs_dev': np.median(np.abs(ts - np.median(ts)))\n",
    "        }\n",
    "    \n",
    "    # ==================== NEW FAST ESTIMATORS ====================\n",
    "    \n",
    "    def _estimate_ma_process_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast MA process estimation using autocorrelation\"\"\"\n",
    "        if len(ts) < 5:\n",
    "            return {'ma_coef': 0.0, 'ma_resid_std': np.std(ts)}\n",
    "        \n",
    "        # Use first autocorrelation to estimate MA(1) coefficient\n",
    "        acf_1 = np.corrcoef(ts[:-1], ts[1:])[0, 1]\n",
    "        \n",
    "        # Solve for theta in MA(1): ρ(1) = θ/(1+θ²)\n",
    "        # This gives a quadratic equation: θ²ρ(1) - θ + ρ(1) = 0\n",
    "        if abs(acf_1) < 0.5:\n",
    "            # Use the invertible solution\n",
    "            theta = (1 - np.sqrt(1 - 4 * acf_1**2)) / (2 * acf_1) if abs(acf_1) > 1e-10 else 0.0\n",
    "        else:\n",
    "            theta = 0.0  # Not invertible, set to 0\n",
    "            \n",
    "        return {'ma_coef': theta, 'ma_resid_std': np.std(ts)}\n",
    "    \n",
    "    def _estimate_ar_process_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast AR process estimation (beyond AR1) using Yule-Walker\"\"\"\n",
    "        n = len(ts)\n",
    "        if n < 6:\n",
    "            return {'ar2_coef1': 0.0, 'ar2_coef2': 0.0, 'ar_resid_std': np.std(ts)}\n",
    "        \n",
    "        # Estimate AR(2) parameters using Yule-Walker equations\n",
    "        acf_0 = 1.0\n",
    "        acf_1 = np.corrcoef(ts[:-1], ts[1:])[0, 1]\n",
    "        acf_2 = np.corrcoef(ts[:-2], ts[2:])[0, 1] if n > 5 else 0\n",
    "        \n",
    "        # Solve Yule-Walker equations for AR(2)\n",
    "        denominator = 1 - acf_1**2\n",
    "        if denominator > 1e-10:\n",
    "            phi1 = acf_1 * (1 - acf_2) / denominator\n",
    "            phi2 = (acf_2 - acf_1**2) / denominator\n",
    "        else:\n",
    "            phi1, phi2 = 0.0, 0.0\n",
    "            \n",
    "        return {'ar2_coef1': phi1, 'ar2_coef2': phi2, 'ar_resid_std': np.std(ts)}\n",
    "    \n",
    "    def _estimate_arma_process_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast ARMA(1,1) estimation using moments\"\"\"\n",
    "        n = len(ts)\n",
    "        if n < 6:\n",
    "            return {'arma_phi': 0.0, 'arma_theta': 0.0, 'arma_resid_std': np.std(ts)}\n",
    "        \n",
    "        # Use first two autocorrelations to estimate ARMA(1,1)\n",
    "        acf_1 = np.corrcoef(ts[:-1], ts[1:])[0, 1]\n",
    "        acf_2 = np.corrcoef(ts[:-2], ts[2:])[0, 1] if n > 5 else 0\n",
    "        \n",
    "        # For ARMA(1,1): ρ₂ = φ * ρ₁\n",
    "        if abs(acf_1) > 1e-10:\n",
    "            phi = acf_2 / acf_1 if abs(acf_1) > abs(acf_2) else 0.0\n",
    "            # Solve for theta using ρ₁ = (1+φθ)(φ+θ)/(1+2φθ+θ²)\n",
    "            # We'll use a simple approximation\n",
    "            theta = (acf_1 - phi) / (1 - phi * acf_1) if abs(phi) < 0.99 else 0.0\n",
    "        else:\n",
    "            phi, theta = 0.0, 0.0\n",
    "            \n",
    "        return {'arma_phi': phi, 'arma_theta': theta, 'arma_resid_std': np.std(ts)}\n",
    "    \n",
    "    def _estimate_garch_process_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast GARCH(1,1) estimation using moments\"\"\"\n",
    "        if len(ts) < 10:\n",
    "            return {'garch_alpha': 0.0, 'garch_beta': 0.0, 'garch_omega': np.var(ts)}\n",
    "        \n",
    "        returns = np.diff(ts)\n",
    "        squared_returns = returns ** 2\n",
    "        \n",
    "        # Estimate parameters using moment matching\n",
    "        mean_sq = np.mean(squared_returns)\n",
    "        var_sq = np.var(squared_returns)\n",
    "        \n",
    "        # Simplified GARCH(1,1) estimation\n",
    "        if var_sq > 1e-10:\n",
    "            # Use the relationship: E[ε²] = ω/(1-α-β)\n",
    "            # and Var[ε²] = ω²(1 + (α+β)² + α²(κ-1)) / [(1-α-β)²(1-(α+β)²-α²(κ-1))]\n",
    "            # For simplicity, we use a heuristic approach\n",
    "            alpha = min(0.1, 0.9 * (var_sq / (mean_sq**2 + var_sq)))\n",
    "            beta = min(0.85, 0.9 - alpha)\n",
    "            omega = mean_sq * (1 - alpha - beta)\n",
    "        else:\n",
    "            alpha, beta, omega = 0.0, 0.0, mean_sq\n",
    "            \n",
    "        return {'garch_alpha': alpha, 'garch_beta': beta, 'garch_omega': omega}\n",
    "    \n",
    "    def _estimate_ornstein_uhlenbeck_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast Ornstein-Uhlenbeck process estimation\"\"\"\n",
    "        if len(ts) < 5:\n",
    "            return {'ou_theta': 0.0, 'ou_mu': np.mean(ts), 'ou_sigma': np.std(ts)}\n",
    "        \n",
    "        # Estimate using linear regression between Δx and x\n",
    "        x_t = ts[:-1]\n",
    "        dx = np.diff(ts)\n",
    "        \n",
    "        # Linear regression: dx = θ(μ - x)dt + σdW\n",
    "        # Which can be written as: dx = (θμ)dt - θ x dt + σdW\n",
    "        x_mean = np.mean(x_t)\n",
    "        dx_mean = np.mean(dx)\n",
    "        \n",
    "        numerator = np.sum((x_t - x_mean) * (dx - dx_mean))\n",
    "        denominator = np.sum((x_t - x_mean) ** 2)\n",
    "        \n",
    "        if denominator > 1e-10:\n",
    "            theta = -numerator / denominator\n",
    "            mu = (dx_mean + theta * x_mean) / theta if abs(theta) > 1e-10 else x_mean\n",
    "        else:\n",
    "            theta, mu = 0.0, x_mean\n",
    "            \n",
    "        # Estimate sigma from residuals\n",
    "        residuals = dx - (theta * (mu - x_t))\n",
    "        sigma = np.std(residuals, ddof=1) if len(residuals) > 1 else np.std(ts)\n",
    "        \n",
    "        return {'ou_theta': theta, 'ou_mu': mu, 'ou_sigma': sigma}\n",
    "    \n",
    "    def _estimate_geometric_brownian_motion_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast Geometric Brownian Motion estimation\"\"\"\n",
    "        if len(ts) < 3:\n",
    "            return {'gbm_mu': 0.0, 'gbm_sigma': np.std(ts)}\n",
    "        \n",
    "        # Calculate log returns\n",
    "        log_returns = np.diff(np.log(ts + 1e-10))  # Add small constant to avoid log(0)\n",
    "        \n",
    "        # Estimate parameters\n",
    "        mu = np.mean(log_returns) + 0.5 * np.var(log_returns)\n",
    "        sigma = np.std(log_returns, ddof=1)\n",
    "        \n",
    "        return {'gbm_mu': mu, 'gbm_sigma': sigma}\n",
    "    \n",
    "    def _estimate_levy_stable_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast Lévy stable distribution estimation using quantile method\"\"\"\n",
    "        if len(ts) < 10:\n",
    "            return {'levy_alpha': 2.0, 'levy_beta': 0.0, 'levy_scale': np.std(ts)}\n",
    "        \n",
    "        # Use quantile method to estimate parameters\n",
    "        q05, q25, q50, q75, q95 = np.percentile(ts, [5, 25, 50, 75, 95])\n",
    "        \n",
    "        # Estimate alpha (characteristic exponent)\n",
    "        if abs(q95 - q05) > 1e-10:\n",
    "            alpha_est = (q95 - q05) / (q75 - q25 + 1e-10)\n",
    "            alpha = np.log2(np.log(alpha_est)) if alpha_est > 0 else 2.0  # Simplified approach\n",
    "            alpha = max(0.5, min(2.0, alpha))  # Constrain to valid range\n",
    "        else:\n",
    "            alpha = 2.0\n",
    "            \n",
    "        # Estimate beta (skewness parameter)\n",
    "        if abs(q95 - q50) > 1e-10 and abs(q50 - q05) > 1e-10:\n",
    "            beta_est = (q95 - q50) / (q50 - q05) - 1\n",
    "            beta = max(-1.0, min(1.0, beta_est))  # Constrain to valid range\n",
    "        else:\n",
    "            beta = 0.0\n",
    "            \n",
    "        # Estimate scale (similar to standard deviation for alpha=2)\n",
    "        scale = (q75 - q25) / 1.349  # For normal distribution, IQR/1.349 is std\n",
    "        \n",
    "        return {'levy_alpha': alpha, 'levy_beta': beta, 'levy_scale': scale}\n",
    "    \n",
    "    # ==================== FAST STATISTICAL FUNCTIONS ====================\n",
    "    \n",
    "    def _skewness_fast(self, x: np.ndarray) -> float:\n",
    "        \"\"\"Fast skewness calculation\"\"\"\n",
    "        n = len(x)\n",
    "        if n < 3:\n",
    "            return 0.0\n",
    "            \n",
    "        mean = np.mean(x)\n",
    "        std = np.std(x, ddof=1)\n",
    "        \n",
    "        if std < 1e-10:\n",
    "            return 0.0\n",
    "            \n",
    "        skew = np.mean(((x - mean) / std) ** 3)\n",
    "        return skew if np.isfinite(skew) else 0.0\n",
    "    \n",
    "    def _kurtosis_fast(self, x: np.ndarray) -> float:\n",
    "        \"\"\"Fast kurtosis calculation\"\"\"\n",
    "        n = len(x)\n",
    "        if n < 4:\n",
    "            return 0.0\n",
    "            \n",
    "        mean = np.mean(x)\n",
    "        std = np.std(x, ddof=1)\n",
    "        \n",
    "        if std < 1e-10:\n",
    "            return 0.0\n",
    "            \n",
    "        kurt = np.mean(((x - mean) / std) ** 4) - 3  # Excess kurtosis\n",
    "        return kurt if np.isfinite(kurt) else 0.0\n",
    "\n",
    "\n",
    "\n",
    "    def _estimate_white_noise_t_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast estimation of t-distributed white noise parameters\"\"\"\n",
    "        if len(ts) < 10:\n",
    "            return {'df': 2.0, 'mean': np.mean(ts), 'scale': np.std(ts, ddof=1)}\n",
    "        \n",
    "        # Use method of moments to estimate degrees of freedom\n",
    "        kurt = self._kurtosis_fast(ts)\n",
    "        if kurt > 0:\n",
    "            df = 6 / kurt + 4\n",
    "        else:\n",
    "            df = 2.0  # Minimum reasonable value\n",
    "            \n",
    "        return {'df': max(2.0, min(df, 30.0)), 'mean': np.mean(ts), 'scale': np.std(ts, ddof=1)}\n",
    "\n",
    "    def _estimate_threshold_ar_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast estimation of threshold autoregressive model parameters\"\"\"\n",
    "        if len(ts) < 20:\n",
    "            return {'threshold': 0.0, 'ar_low': 0.0, 'ar_high': 0.0}\n",
    "        \n",
    "        # Simple threshold estimation using median\n",
    "        threshold = np.median(ts)\n",
    "        \n",
    "        # Split series based on threshold\n",
    "        low_regime = ts[:-1][ts[:-1] <= threshold]\n",
    "        high_regime = ts[:-1][ts[:-1] > threshold]\n",
    "        \n",
    "        # Estimate AR(1) for each regime\n",
    "        ar_low = self._estimate_ar1_fast(np.concatenate([low_regime, ts[1:][ts[:-1] <= threshold]]))['phi']\n",
    "        ar_high = self._estimate_ar1_fast(np.concatenate([high_regime, ts[1:][ts[:-1] > threshold]]))['phi']\n",
    "        \n",
    "        return {'threshold': threshold, 'ar_low': ar_low, 'ar_high': ar_high}\n",
    "\n",
    "    def _estimate_sinusoidal_with_noise_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast estimation of sinusoidal process with noise\"\"\"\n",
    "        if len(ts) < 10:\n",
    "            return {'amplitude': 0.0, 'frequency': 0.0, 'phase': 0.0, 'noise_std': np.std(ts)}\n",
    "        \n",
    "        # Use FFT to estimate dominant frequency\n",
    "        n = len(ts)\n",
    "        fft_vals = np.fft.fft(ts - np.mean(ts))\n",
    "        freqs = np.fft.fftfreq(n)\n",
    "        \n",
    "        # Find dominant frequency (excluding DC component)\n",
    "        idx = np.argmax(np.abs(fft_vals[1:n//2])) + 1\n",
    "        dominant_freq = abs(freqs[idx])\n",
    "        \n",
    "        # Estimate amplitude\n",
    "        amplitude = 2 * np.abs(fft_vals[idx]) / n\n",
    "        \n",
    "        # Estimate phase\n",
    "        phase = np.angle(fft_vals[idx])\n",
    "        \n",
    "        # Estimate noise standard deviation\n",
    "        # Create sinusoidal approximation\n",
    "        t = np.arange(n)\n",
    "        sinusoid = amplitude * np.cos(2 * np.pi * dominant_freq * t + phase)\n",
    "        residuals = ts - sinusoid\n",
    "        noise_std = np.std(residuals, ddof=1)\n",
    "        \n",
    "        return {\n",
    "            'amplitude': amplitude,\n",
    "            'frequency': dominant_freq,\n",
    "            'phase': phase,\n",
    "            'noise_std': noise_std\n",
    "        }\n",
    "\n",
    "    def _estimate_logistic_growth_fast(self, ts: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Fast estimation of logistic growth parameters\"\"\"\n",
    "        if len(ts) < 10:\n",
    "            return {'carrying_capacity': np.max(ts), 'growth_rate': 0.0, 'midpoint': 0.0}\n",
    "        \n",
    "        # Simple estimation using first and last values\n",
    "        initial = ts[0]\n",
    "        final = ts[-1]\n",
    "        \n",
    "        # Estimate carrying capacity as maximum value\n",
    "        carrying_capacity = np.max(ts)\n",
    "        \n",
    "        # Estimate growth rate using simplified approach\n",
    "        if initial > 0 and final > initial:\n",
    "            growth_rate = np.log(final / initial) / len(ts)\n",
    "        else:\n",
    "            growth_rate = 0.0\n",
    "            \n",
    "        # Estimate midpoint as time when growth is fastest\n",
    "        diffs = np.diff(ts)\n",
    "        midpoint_idx = np.argmax(diffs)\n",
    "        midpoint = midpoint_idx / len(ts) if len(ts) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'carrying_capacity': carrying_capacity,\n",
    "            'growth_rate': growth_rate,\n",
    "            'midpoint': midpoint\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0da7f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import scipy.stats\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, coint\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from typing import Iterable, Iterator\n",
    "from numba import jit, prange\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import zlib\n",
    "import gzip\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Numba-optimized functions for speed\n",
    "@jit(nopython=True)\n",
    "def fast_autocorr(x, max_lag=5):\n",
    "    \"\"\"Fast autocorrelation calculation using numba\"\"\"\n",
    "    n = len(x)\n",
    "    if n < max_lag + 1:\n",
    "        return np.zeros(max_lag)\n",
    "\n",
    "    x = x - np.mean(x)\n",
    "    autocorrs = np.zeros(max_lag)\n",
    "\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        if lag < n:\n",
    "            c0 = np.sum(x**2) / n\n",
    "            c_lag = np.sum(x[:-lag] * x[lag:]) / n\n",
    "            if c0 > 0:\n",
    "                autocorrs[lag-1] = c_lag / c0\n",
    "\n",
    "    return autocorrs\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_rolling_std(x, window=10):\n",
    "    \"\"\"Fast rolling standard deviation\"\"\"\n",
    "    n = len(x)\n",
    "    if n < window:\n",
    "        return np.std(x)\n",
    "\n",
    "    stds = np.zeros(n - window + 1)\n",
    "    for i in range(n - window + 1):\n",
    "        stds[i] = np.std(x[i:i+window])\n",
    "\n",
    "    return np.mean(stds)\n",
    "\n",
    "def extract_dtw_features(values_before, values_after):\n",
    "    \"\"\"Extract Dynamic Time Warping distance features\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        if len(values_before) > 0 and len(values_after) > 0:\n",
    "            # Normalize sequences for DTW\n",
    "            before_norm = (values_before - values_before.mean()) / (values_before.std() + 1e-10)\n",
    "            after_norm = (values_after - values_after.mean()) / (values_after.std() + 1e-10)\n",
    "\n",
    "            # DTW distance\n",
    "            dtw_distance = dtw.distance(before_norm.values, after_norm.values)\n",
    "\n",
    "            # Normalize by sequence length\n",
    "            avg_length = (len(values_before) + len(values_after)) / 2\n",
    "            features[\"dtw_distance\"] = dtw_distance / avg_length\n",
    "\n",
    "            # DTW distance with different window sizes\n",
    "            for window in [0.1, 0.2, 0.5]:\n",
    "                try:\n",
    "                    window_size = int(window * avg_length)\n",
    "                    if window_size > 0:\n",
    "                        dtw_windowed = dtw.distance(before_norm.values, after_norm.values,\n",
    "                                                  window=window_size)\n",
    "                        features[f\"dtw_window_{int(window*100)}\"] = dtw_windowed / avg_length\n",
    "                except:\n",
    "                    features[f\"dtw_window_{int(window*100)}\"] = 0\n",
    "        else:\n",
    "            features[\"dtw_distance\"] = 0\n",
    "            for window in [10, 20, 50]:\n",
    "                features[f\"dtw_window_{window}\"] = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"DTW calculation failed: {e}\")\n",
    "        features[\"dtw_distance\"] = 0\n",
    "        for window in [10, 20, 50]:\n",
    "            features[f\"dtw_window_{window}\"] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_compression_features(values_before, values_after):\n",
    "    \"\"\"Extract Compression-Based Dissimilarity (CBD) features\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        # Convert to bytes for compression\n",
    "        before_bytes = values_before.values.tobytes()\n",
    "        after_bytes = values_after.values.tobytes()\n",
    "        combined_bytes = np.concatenate([values_before.values, values_after.values]).tobytes()\n",
    "\n",
    "        # Compression lengths using different algorithms\n",
    "        for compressor, name in [(zlib.compress, 'zlib'), (gzip.compress, 'gzip')]:\n",
    "            try:\n",
    "                c_before = len(compressor(before_bytes))\n",
    "                c_after = len(compressor(after_bytes))\n",
    "                c_combined = len(compressor(combined_bytes))\n",
    "\n",
    "                # Normalized Compression Distance (NCD)\n",
    "                ncd = (c_combined - min(c_before, c_after)) / max(c_before, c_after)\n",
    "                features[f\"ncd_{name}\"] = ncd\n",
    "\n",
    "                # Compression ratios\n",
    "                features[f\"compression_ratio_before_{name}\"] = c_before / len(before_bytes)\n",
    "                features[f\"compression_ratio_after_{name}\"] = c_after / len(after_bytes)\n",
    "\n",
    "            except Exception as e:\n",
    "                features[f\"ncd_{name}\"] = 0\n",
    "                features[f\"compression_ratio_before_{name}\"] = 1\n",
    "                features[f\"compression_ratio_after_{name}\"] = 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Compression features failed: {e}\")\n",
    "        for name in ['zlib', 'gzip']:\n",
    "            features[f\"ncd_{name}\"] = 0\n",
    "            features[f\"compression_ratio_before_{name}\"] = 1\n",
    "            features[f\"compression_ratio_after_{name}\"] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_cointegration_features(values_before, values_after):\n",
    "    \"\"\"Extract cointegration features\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        if len(values_before) > 10 and len(values_after) > 10:\n",
    "            # Test for cointegration between the two periods\n",
    "            # Pad shorter series to match lengths for cointegration test\n",
    "            min_len = min(len(values_before), len(values_after))\n",
    "            before_trimmed = values_before.iloc[:min_len]\n",
    "            after_trimmed = values_after.iloc[:min_len]\n",
    "\n",
    "            if min_len > 12:  # Need sufficient data for cointegration test\n",
    "                # Engle-Granger cointegration test\n",
    "                coint_stat, coint_pvalue, _ = coint(before_trimmed, after_trimmed)\n",
    "                features[\"cointegration_stat\"] = coint_stat\n",
    "                features[\"cointegration_pvalue\"] = coint_pvalue\n",
    "                features[\"cointegration_evidence\"] = 1 if coint_pvalue < 0.05 else 0\n",
    "            else:\n",
    "                features[\"cointegration_stat\"] = 0\n",
    "                features[\"cointegration_pvalue\"] = 1\n",
    "                features[\"cointegration_evidence\"] = 0\n",
    "        else:\n",
    "            features[\"cointegration_stat\"] = 0\n",
    "            features[\"cointegration_pvalue\"] = 1\n",
    "            features[\"cointegration_evidence\"] = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Cointegration test failed: {e}\")\n",
    "        features[\"cointegration_stat\"] = 0\n",
    "        features[\"cointegration_pvalue\"] = 1\n",
    "        features[\"cointegration_evidence\"] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_chow_test_features(df):\n",
    "    \"\"\"Extract Chow test features for structural break detection\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        values = df[\"value\"].values\n",
    "        time_idx = np.arange(len(values))\n",
    "\n",
    "        if len(values) > 20:  # Need sufficient data\n",
    "            # Find potential breakpoint (midpoint for now, but could be optimized)\n",
    "            breakpoint = len(values) // 2\n",
    "\n",
    "            # Fit models for full sample and subsamples\n",
    "            # Full sample regression\n",
    "            X_full = np.column_stack([np.ones(len(values)), time_idx])\n",
    "            try:\n",
    "                beta_full = np.linalg.lstsq(X_full, values, rcond=None)[0]\n",
    "                residuals_full = values - X_full @ beta_full\n",
    "                rss_full = np.sum(residuals_full**2)\n",
    "\n",
    "                # First subsample\n",
    "                X1 = np.column_stack([np.ones(breakpoint), time_idx[:breakpoint]])\n",
    "                beta1 = np.linalg.lstsq(X1, values[:breakpoint], rcond=None)[0]\n",
    "                residuals1 = values[:breakpoint] - X1 @ beta1\n",
    "                rss1 = np.sum(residuals1**2)\n",
    "\n",
    "                # Second subsample\n",
    "                X2 = np.column_stack([np.ones(len(values) - breakpoint),\n",
    "                                    time_idx[breakpoint:]])\n",
    "                beta2 = np.linalg.lstsq(X2, values[breakpoint:], rcond=None)[0]\n",
    "                residuals2 = values[breakpoint:] - X2 @ beta2\n",
    "                rss2 = np.sum(residuals2**2)\n",
    "\n",
    "                # Chow test statistic\n",
    "                k = 2  # number of parameters\n",
    "                n = len(values)\n",
    "                chow_stat = ((rss_full - (rss1 + rss2)) / k) / ((rss1 + rss2) / (n - 2*k))\n",
    "\n",
    "                features[\"chow_statistic\"] = chow_stat\n",
    "                features[\"chow_pvalue\"] = 1 - scipy.stats.f.cdf(chow_stat, k, n - 2*k)\n",
    "\n",
    "                # Parameter differences\n",
    "                features[\"slope_diff\"] = abs(beta1[1] - beta2[1]) if len(beta1) > 1 and len(beta2) > 1 else 0\n",
    "                features[\"intercept_diff\"] = abs(beta1[0] - beta2[0])\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                features[\"chow_statistic\"] = 0\n",
    "                features[\"chow_pvalue\"] = 1\n",
    "                features[\"slope_diff\"] = 0\n",
    "                features[\"intercept_diff\"] = 0\n",
    "        else:\n",
    "            features[\"chow_statistic\"] = 0\n",
    "            features[\"chow_pvalue\"] = 1\n",
    "            features[\"slope_diff\"] = 0\n",
    "            features[\"intercept_diff\"] = 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Chow test failed: {e}\")\n",
    "        features[\"chow_statistic\"] = 0\n",
    "        features[\"chow_pvalue\"] = 1\n",
    "        features[\"slope_diff\"] = 0\n",
    "        features[\"intercept_diff\"] = 0\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_likelihood_ratio_features(df):\n",
    "    \"\"\"Extract likelihood ratio test features\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    try:\n",
    "        values_before = df[\"value\"][df[\"period\"] == 0]\n",
    "        values_after = df[\"value\"][df[\"period\"] == 1]\n",
    "\n",
    "        if len(values_before) > 5 and len(values_after) > 5:\n",
    "            # Likelihood ratio for normal distributions\n",
    "            # H0: same parameters, H1: different parameters\n",
    "\n",
    "            # Combined sample\n",
    "            all_values = np.concatenate([values_before, values_after])\n",
    "            mu_combined = np.mean(all_values)\n",
    "            sigma_combined = np.std(all_values, ddof=1)\n",
    "\n",
    "            # Separate samples\n",
    "            mu_before = np.mean(values_before)\n",
    "            sigma_before = np.std(values_before, ddof=1)\n",
    "            mu_after = np.mean(values_after)\n",
    "            sigma_after = np.std(values_after, ddof=1)\n",
    "\n",
    "            # Log-likelihoods\n",
    "            n_before = len(values_before)\n",
    "            n_after = len(values_after)\n",
    "            n_total = n_before + n_after\n",
    "\n",
    "            # Combined model likelihood\n",
    "            if sigma_combined > 0:\n",
    "                ll_combined = -n_total/2 * np.log(2*np.pi) - n_total * np.log(sigma_combined) - \\\n",
    "                             np.sum((all_values - mu_combined)**2) / (2 * sigma_combined**2)\n",
    "            else:\n",
    "                ll_combined = 0\n",
    "\n",
    "            # Separate models likelihood\n",
    "            ll_separate = 0\n",
    "            if sigma_before > 0:\n",
    "                ll_separate += -n_before/2 * np.log(2*np.pi) - n_before * np.log(sigma_before) - \\\n",
    "                              np.sum((values_before - mu_before)**2) / (2 * sigma_before**2)\n",
    "            if sigma_after > 0:\n",
    "                ll_separate += -n_after/2 * np.log(2*np.pi) - n_after * np.log(sigma_after) - \\\n",
    "                              np.sum((values_after - mu_after)**2) / (2 * sigma_after**2)\n",
    "\n",
    "            # Likelihood ratio statistic\n",
    "            lr_stat = -2 * (ll_combined - ll_separate)\n",
    "            features[\"lr_statistic\"] = lr_stat\n",
    "            features[\"lr_pvalue\"] = 1 - scipy.stats.chi2.cdf(lr_stat, df=2)  # 2 df for mean and variance\n",
    "\n",
    "        else:\n",
    "            features[\"lr_statistic\"] = 0\n",
    "            features[\"lr_pvalue\"] = 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Likelihood ratio test failed: {e}\")\n",
    "        features[\"lr_statistic\"] = 0\n",
    "        features[\"lr_pvalue\"] = 1\n",
    "\n",
    "    return features\n",
    "def extract_difference_features(before_stats, after_stats):\n",
    "    \"\"\"\n",
    "    Calculate differences and ratios between before and after statistics\n",
    "    \"\"\"\n",
    "    diff_features = {}\n",
    "\n",
    "    # Calculate differences\n",
    "    for key in before_stats:\n",
    "        # Absolute difference\n",
    "        diff_features[f\"{key}_diff\"] = after_stats[key] - before_stats[key]\n",
    "\n",
    "        # Percentage change (avoid division by zero)\n",
    "        if abs(before_stats[key]) > 1e-10:\n",
    "            diff_features[f\"{key}_pct_change\"] = (after_stats[key] - before_stats[key]) / abs(before_stats[key])\n",
    "        else:\n",
    "            diff_features[f\"{key}_pct_change\"] = 0\n",
    "\n",
    "        # Absolute ratio (use log for better scaling)\n",
    "        if abs(before_stats[key]) > 1e-10 and abs(after_stats[key]) > 1e-10:\n",
    "            ratio = after_stats[key] / before_stats[key]\n",
    "            if ratio > 0:  # Ensure log is defined\n",
    "                diff_features[f\"{key}_log_ratio\"] = np.log(ratio)\n",
    "            else:\n",
    "                diff_features[f\"{key}_log_ratio\"] = 0\n",
    "        else:\n",
    "            diff_features[f\"{key}_log_ratio\"] = 0\n",
    "\n",
    "    return diff_features\n",
    "def extract_enhanced_statistical_features(u: pd.DataFrame):\n",
    "    \"\"\"Enhanced version of statistical test features with optimizations\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    values_before = u[\"value\"][u[\"period\"] == 0]\n",
    "    values_after = u[\"value\"][u[\"period\"] == 1]\n",
    "\n",
    "    if values_before.empty or values_after.empty:\n",
    "        return {f: 0.0 for f in [\n",
    "            \"t_test\", \"ks_test\", \"mann_whitney\", \"anderson_darling\",\n",
    "            \"wasserstein\", \"energy_distance\", \"levene_test\", \"chi_square\",\n",
    "            \"anova\", \"jensen_shannon\"\n",
    "        ]}\n",
    "\n",
    "    # Vectorized statistical tests\n",
    "    before_vals = values_before.values\n",
    "    after_vals = values_after.values\n",
    "\n",
    "    # T-test (optimized)\n",
    "    try:\n",
    "        t_stat, t_pvalue = scipy.stats.ttest_ind(before_vals, after_vals, equal_var=False)\n",
    "        features[\"t_test\"] = -np.log10(max(t_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"t_test\"] = 0.0\n",
    "\n",
    "    # Mann-Whitney U test (more robust than t-test)\n",
    "    try:\n",
    "        mw_stat, mw_pvalue = scipy.stats.mannwhitneyu(before_vals, after_vals, alternative='two-sided')\n",
    "        features[\"mann_whitney\"] = -np.log10(max(mw_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"mann_whitney\"] = 0.0\n",
    "\n",
    "    # KS-test\n",
    "    try:\n",
    "        ks_stat, ks_pvalue = scipy.stats.ks_2samp(before_vals, after_vals)\n",
    "        features[\"ks_test\"] = -np.log10(max(ks_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"ks_test\"] = 0.0\n",
    "\n",
    "    # Wasserstein distance (fast implementation)\n",
    "    try:\n",
    "        wasserstein = scipy.stats.wasserstein_distance(before_vals, after_vals)\n",
    "        data_range = max(u[\"value\"].max() - u[\"value\"].min(), 1e-10)\n",
    "        features[\"wasserstein\"] = wasserstein / data_range\n",
    "    except:\n",
    "        features[\"wasserstein\"] = 0.0\n",
    "\n",
    "    # Energy distance\n",
    "    try:\n",
    "        energy_dist = scipy.stats.energy_distance(before_vals, after_vals)\n",
    "        data_range = max(u[\"value\"].max() - u[\"value\"].min(), 1e-10)\n",
    "        features[\"energy_distance\"] = energy_dist / data_range\n",
    "    except:\n",
    "        features[\"energy_distance\"] = 0.0\n",
    "\n",
    "    # Levene's test\n",
    "    try:\n",
    "        levene_stat, levene_pvalue = scipy.stats.levene(before_vals, after_vals)\n",
    "        features[\"levene_test\"] = -np.log10(max(levene_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"levene_test\"] = 0.0\n",
    "\n",
    "    # ANOVA\n",
    "    try:\n",
    "        f_stat, anova_pvalue = scipy.stats.f_oneway(before_vals, after_vals)\n",
    "        features[\"anova\"] = -np.log10(max(anova_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"anova\"] = 0.0\n",
    "\n",
    "    # Additional robust tests\n",
    "    # Mood's median test\n",
    "    try:\n",
    "        mood_stat, mood_pvalue = scipy.stats.median_test(before_vals, after_vals)\n",
    "        features[\"mood_median\"] = -np.log10(max(mood_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"mood_median\"] = 0.0\n",
    "\n",
    "    # Ansari-Bradley test for scale differences\n",
    "    try:\n",
    "        ab_stat, ab_pvalue = scipy.stats.ansari(before_vals, after_vals)\n",
    "        features[\"ansari_bradley\"] = -np.log10(max(ab_pvalue, 1e-10))\n",
    "    except:\n",
    "        features[\"ansari_bradley\"] = 0.0\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_optimized_descriptive_statistics(values):\n",
    "    \"\"\"Optimized descriptive statistics extraction\"\"\"\n",
    "    if len(values) == 0:\n",
    "        return {f: 0 for f in [\n",
    "            \"mean\", \"median\", \"std\", \"min\", \"max\", \"iqr\", \"skew\",\n",
    "            \"kurtosis\", \"range\", \"entropy\", \"cv\", \"mad\"\n",
    "        ]}\n",
    "\n",
    "    vals = values.values if hasattr(values, 'values') else values\n",
    "\n",
    "    stats = {}\n",
    "    stats[\"mean\"] = np.mean(vals)\n",
    "    stats[\"median\"] = np.median(vals)\n",
    "    stats[\"std\"] = np.std(vals, ddof=1) if len(vals) > 1 else 0\n",
    "    stats[\"min\"] = np.min(vals)\n",
    "    stats[\"max\"] = np.max(vals)\n",
    "\n",
    "    # Quantiles (vectorized)\n",
    "    q25, q75 = np.percentile(vals, [25, 75])\n",
    "    stats[\"iqr\"] = q75 - q25\n",
    "    stats[\"range\"] = stats[\"max\"] - stats[\"min\"]\n",
    "\n",
    "    # Higher order moments\n",
    "    if len(vals) > 2:\n",
    "        stats[\"skew\"] = scipy.stats.skew(vals)\n",
    "        stats[\"kurtosis\"] = scipy.stats.kurtosis(vals)\n",
    "    else:\n",
    "        stats[\"skew\"] = 0\n",
    "        stats[\"kurtosis\"] = 0\n",
    "\n",
    "    # Additional robust statistics\n",
    "    stats[\"cv\"] = stats[\"std\"] / max(abs(stats[\"mean\"]), 1e-10)  # Coefficient of variation\n",
    "    stats[\"mad\"] = np.median(np.abs(vals - stats[\"median\"]))  # Median absolute deviation\n",
    "\n",
    "    return stats\n",
    "\n",
    "def extract_optimized_time_series_features(values):\n",
    "    \"\"\"Optimized time series features using numba acceleration\"\"\"\n",
    "    if len(values) < 10:\n",
    "        return {\"autocorr_lag1\": 0, \"autocorr_lag2\": 0, \"autocorr_lag3\": 0,\n",
    "                \"trend_slope\": 0, \"volatility\": 0, \"hurst_exponent\": 0}\n",
    "\n",
    "    vals = values.values if hasattr(values, 'values') else np.array(values)\n",
    "    features = {}\n",
    "\n",
    "    # Fast autocorrelations\n",
    "    autocorrs = fast_autocorr(vals, max_lag=5)\n",
    "    for i, lag in enumerate([1, 2, 3, 4, 5]):\n",
    "        if i < len(autocorrs):\n",
    "            features[f\"autocorr_lag{lag}\"] = autocorrs[i]\n",
    "        else:\n",
    "            features[f\"autocorr_lag{lag}\"] = 0\n",
    "\n",
    "    # Linear trend (optimized)\n",
    "    x = np.arange(len(vals), dtype=np.float64)\n",
    "    try:\n",
    "        slope = np.polyfit(x, vals, 1)[0]\n",
    "        features[\"trend_slope\"] = slope\n",
    "    except:\n",
    "        features[\"trend_slope\"] = 0\n",
    "\n",
    "    # Volatility measures\n",
    "    if len(vals) > 1:\n",
    "        returns = np.diff(vals) / (vals[:-1] + 1e-10)\n",
    "        features[\"volatility\"] = np.std(returns)\n",
    "        features[\"rolling_volatility\"] = fast_rolling_std(returns, window=min(10, len(returns)//2))\n",
    "    else:\n",
    "        features[\"volatility\"] = 0\n",
    "        features[\"rolling_volatility\"] = 0\n",
    "\n",
    "    # Hurst exponent (simplified version)\n",
    "    try:\n",
    "        if len(vals) > 20:\n",
    "            lags = np.arange(2, min(20, len(vals)//4))\n",
    "            tau = [np.sqrt(np.std(np.subtract(vals[lag:], vals[:-lag]))) for lag in lags]\n",
    "            poly_coef = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "            features[\"hurst_exponent\"] = poly_coef[0]\n",
    "        else:\n",
    "            features[\"hurst_exponent\"] = 0.5\n",
    "    except:\n",
    "        features[\"hurst_exponent\"] = 0.5\n",
    "\n",
    "    return features\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from scipy.fft import fft, fftfreq, ifft\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import wasserstein_distance\n",
    "import pywt\n",
    "\n",
    "\n",
    "def extract_fourier_spectrum_features(values_before, values_after):\n",
    "    \"\"\"Extract Fourier Spectrum Distance features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Ensure equal length for comparison by padding with zeros\n",
    "        max_len = max(len(values_before), len(values_after))\n",
    "        before_padded = np.pad(values_before.values, (0, max_len - len(values_before)), mode='constant')\n",
    "        after_padded = np.pad(values_after.values, (0, max_len - len(values_after)), mode='constant')\n",
    "        \n",
    "        # Compute FFT for both segments\n",
    "        fft_before = fft(before_padded)\n",
    "        fft_after = fft(after_padded)\n",
    "        \n",
    "        # Power spectra (magnitude squared)\n",
    "        power_before = np.abs(fft_before) ** 2\n",
    "        power_after = np.abs(fft_after) ** 2\n",
    "        \n",
    "        # Normalize power spectra\n",
    "        power_before_norm = power_before / np.sum(power_before)\n",
    "        power_after_norm = power_after / np.sum(power_after)\n",
    "        \n",
    "        # L2 norm distance between power spectra\n",
    "        features[\"power_spectrum_l2_distance\"] = np.linalg.norm(power_before_norm - power_after_norm)\n",
    "        \n",
    "        # Earth Mover's Distance (Wasserstein distance)\n",
    "        try:\n",
    "            features[\"power_spectrum_emd\"] = wasserstein_distance(\n",
    "                range(len(power_before_norm)), range(len(power_after_norm)),\n",
    "                power_before_norm, power_after_norm\n",
    "            )\n",
    "        except:\n",
    "            features[\"power_spectrum_emd\"] = 0\n",
    "            \n",
    "        # Spectral centroid difference\n",
    "        freqs = np.arange(len(power_before_norm))\n",
    "        centroid_before = np.sum(freqs * power_before_norm) / np.sum(power_before_norm)\n",
    "        centroid_after = np.sum(freqs * power_after_norm) / np.sum(power_after_norm)\n",
    "        features[\"spectral_centroid_diff\"] = abs(centroid_after - centroid_before)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fourier spectrum features failed: {e}\")\n",
    "        features[\"power_spectrum_l2_distance\"] = 0\n",
    "        features[\"power_spectrum_emd\"] = 0\n",
    "        features[\"spectral_centroid_diff\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_dominant_frequency_features(values_before, values_after):\n",
    "    \"\"\"Extract Dominant Frequency Shift features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Sampling frequency (assumed to be 1 for simplicity)\n",
    "        fs = 1.0\n",
    "        \n",
    "        # Compute FFT for both segments\n",
    "        fft_before = fft(values_before.values)\n",
    "        fft_after = fft(values_after.values)\n",
    "        \n",
    "        # Get frequency arrays\n",
    "        freqs_before = fftfreq(len(values_before), 1/fs)\n",
    "        freqs_after = fftfreq(len(values_after), 1/fs)\n",
    "        \n",
    "        # Find dominant frequencies (max amplitude)\n",
    "        dominant_idx_before = np.argmax(np.abs(fft_before[:len(fft_before)//2]))\n",
    "        dominant_idx_after = np.argmax(np.abs(fft_after[:len(fft_after)//2]))\n",
    "        \n",
    "        dominant_freq_before = abs(freqs_before[dominant_idx_before])\n",
    "        dominant_freq_after = abs(freqs_after[dominant_idx_after])\n",
    "        \n",
    "        # Dominant frequency shift\n",
    "        features[\"dominant_frequency_shift\"] = abs(dominant_freq_after - dominant_freq_before)\n",
    "        features[\"dominant_frequency_ratio\"] = dominant_freq_after / max(dominant_freq_before, 1e-10)\n",
    "        \n",
    "        # Amplitude of dominant frequency\n",
    "        features[\"dominant_amplitude_before\"] = np.abs(fft_before[dominant_idx_before])\n",
    "        features[\"dominant_amplitude_after\"] = np.abs(fft_after[dominant_idx_after])\n",
    "        features[\"dominant_amplitude_ratio\"] = features[\"dominant_amplitude_after\"] / max(features[\"dominant_amplitude_before\"], 1e-10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Dominant frequency features failed: {e}\")\n",
    "        features[\"dominant_frequency_shift\"] = 0\n",
    "        features[\"dominant_frequency_ratio\"] = 1\n",
    "        features[\"dominant_amplitude_before\"] = 0\n",
    "        features[\"dominant_amplitude_after\"] = 0\n",
    "        features[\"dominant_amplitude_ratio\"] = 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_band_energy_features(values_before, values_after):\n",
    "    \"\"\"Extract Frequency Band Energy features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Define frequency bands (normalized frequencies)\n",
    "        bands = {\n",
    "            'low': (0, 0.1),\n",
    "            'mid': (0.1, 0.3),\n",
    "            'high': (0.3, 0.5)\n",
    "        }\n",
    "        \n",
    "        for segment_name, values in [('before', values_before), ('after', values_after)]:\n",
    "            # Compute power spectral density\n",
    "            freqs, psd = signal.welch(values.values, nperseg=min(len(values), 256))\n",
    "            \n",
    "            # Calculate energy in each band\n",
    "            for band_name, (low_freq, high_freq) in bands.items():\n",
    "                mask = (freqs >= low_freq) & (freqs <= high_freq)\n",
    "                band_energy = np.sum(psd[mask])\n",
    "                features[f\"{segment_name}_energy_{band_name}\"] = band_energy\n",
    "        \n",
    "        # Calculate ratios and differences\n",
    "        total_energy_before = sum(features[f\"before_energy_{band}\"] for band in bands.keys())\n",
    "        total_energy_after = sum(features[f\"after_energy_{band}\"] for band in bands.keys())\n",
    "        \n",
    "        for band_name in bands.keys():\n",
    "            # Energy ratios within each period\n",
    "            features[f\"before_{band_name}_energy_ratio\"] = features[f\"before_energy_{band_name}\"] / max(total_energy_before, 1e-10)\n",
    "            features[f\"after_{band_name}_energy_ratio\"] = features[f\"after_energy_{band_name}\"] / max(total_energy_after, 1e-10)\n",
    "            \n",
    "            # Cross-period comparisons\n",
    "            features[f\"{band_name}_energy_change\"] = features[f\"after_energy_{band_name}\"] - features[f\"before_energy_{band_name}\"]\n",
    "            features[f\"{band_name}_energy_ratio\"] = features[f\"after_energy_{band_name}\"] / max(features[f\"before_energy_{band_name}\"], 1e-10)\n",
    "        \n",
    "        # Low/High frequency ratio change\n",
    "        features[\"low_high_ratio_before\"] = features[\"before_energy_low\"] / max(features[\"before_energy_high\"], 1e-10)\n",
    "        features[\"low_high_ratio_after\"] = features[\"after_energy_low\"] / max(features[\"after_energy_high\"], 1e-10)\n",
    "        features[\"low_high_ratio_change\"] = features[\"low_high_ratio_after\"] - features[\"low_high_ratio_before\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Band energy features failed: {e}\")\n",
    "        for band in ['low', 'mid', 'high']:\n",
    "            for period in ['before', 'after']:\n",
    "                features[f\"{period}_energy_{band}\"] = 0\n",
    "                features[f\"{period}_{band}_energy_ratio\"] = 0\n",
    "            features[f\"{band}_energy_change\"] = 0\n",
    "            features[f\"{band}_energy_ratio\"] = 1\n",
    "        features[\"low_high_ratio_before\"] = 1\n",
    "        features[\"low_high_ratio_after\"] = 1\n",
    "        features[\"low_high_ratio_change\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_wavelet_energy_features(values_before, values_after):\n",
    "    \"\"\"Extract Wavelet Energy Shift features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        wavelet = 'db4'  # Daubechies wavelet\n",
    "        max_level = 5\n",
    "        \n",
    "        for segment_name, values in [('before', values_before), ('after', values_after)]:\n",
    "            # Perform wavelet decomposition\n",
    "            coeffs = pywt.wavedec(values.values, wavelet, level=max_level)\n",
    "            \n",
    "            # Calculate energy at each level\n",
    "            for i, coeff in enumerate(coeffs):\n",
    "                energy = np.sum(coeff ** 2)\n",
    "                if i == 0:\n",
    "                    features[f\"{segment_name}_wavelet_approx_energy\"] = energy\n",
    "                else:\n",
    "                    features[f\"{segment_name}_wavelet_detail_{i}_energy\"] = energy\n",
    "        \n",
    "        # Calculate energy differences and ratios\n",
    "        features[\"wavelet_approx_energy_diff\"] = features[\"after_wavelet_approx_energy\"] - features[\"before_wavelet_approx_energy\"]\n",
    "        features[\"wavelet_approx_energy_ratio\"] = features[\"after_wavelet_approx_energy\"] / max(features[\"before_wavelet_approx_energy\"], 1e-10)\n",
    "        \n",
    "        for i in range(1, max_level + 1):\n",
    "            before_key = f\"before_wavelet_detail_{i}_energy\"\n",
    "            after_key = f\"after_wavelet_detail_{i}_energy\"\n",
    "            \n",
    "            if before_key in features and after_key in features:\n",
    "                features[f\"wavelet_detail_{i}_energy_diff\"] = features[after_key] - features[before_key]\n",
    "                features[f\"wavelet_detail_{i}_energy_ratio\"] = features[after_key] / max(features[before_key], 1e-10)\n",
    "        \n",
    "        # Total detail energy comparison\n",
    "        total_detail_before = sum(features.get(f\"before_wavelet_detail_{i}_energy\", 0) for i in range(1, max_level + 1))\n",
    "        total_detail_after = sum(features.get(f\"after_wavelet_detail_{i}_energy\", 0) for i in range(1, max_level + 1))\n",
    "        \n",
    "        features[\"total_detail_energy_diff\"] = total_detail_after - total_detail_before\n",
    "        features[\"total_detail_energy_ratio\"] = total_detail_after / max(total_detail_before, 1e-10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Wavelet energy features failed: {e}\")\n",
    "        features[\"wavelet_approx_energy_diff\"] = 0\n",
    "        features[\"wavelet_approx_energy_ratio\"] = 1\n",
    "        features[\"total_detail_energy_diff\"] = 0\n",
    "        features[\"total_detail_energy_ratio\"] = 1\n",
    "        for i in range(1, 6):\n",
    "            features[f\"before_wavelet_detail_{i}_energy\"] = 0\n",
    "            features[f\"after_wavelet_detail_{i}_energy\"] = 0\n",
    "            features[f\"wavelet_detail_{i}_energy_diff\"] = 0\n",
    "            features[f\"wavelet_detail_{i}_energy_ratio\"] = 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_spectrogram_difference_features(values_before, values_after):\n",
    "    \"\"\"Extract Time-Frequency Spectrogram features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Parameters for STFT\n",
    "        nperseg = min(32, len(values_before) // 4, len(values_after) // 4)\n",
    "        nperseg = max(nperseg, 8)  # Minimum window size\n",
    "        \n",
    "        # Compute spectrograms\n",
    "        f_before, t_before, Sxx_before = signal.spectrogram(values_before.values, nperseg=nperseg)\n",
    "        f_after, t_after, Sxx_after = signal.spectrogram(values_after.values, nperseg=nperseg)\n",
    "        \n",
    "        # Normalize spectrograms\n",
    "        Sxx_before_norm = Sxx_before / (np.sum(Sxx_before) + 1e-10)\n",
    "        Sxx_after_norm = Sxx_after / (np.sum(Sxx_after) + 1e-10)\n",
    "        \n",
    "        # Resize to same dimensions for comparison\n",
    "        target_shape = (min(Sxx_before.shape[0], Sxx_after.shape[0]), \n",
    "                       min(Sxx_before.shape[1], Sxx_after.shape[1]))\n",
    "        \n",
    "        if target_shape[0] > 0 and target_shape[1] > 0:\n",
    "            Sxx_before_resized = Sxx_before_norm[:target_shape[0], :target_shape[1]]\n",
    "            Sxx_after_resized = Sxx_after_norm[:target_shape[0], :target_shape[1]]\n",
    "            \n",
    "            # Mean absolute difference\n",
    "            features[\"spectrogram_mean_diff\"] = np.mean(np.abs(Sxx_after_resized - Sxx_before_resized))\n",
    "            \n",
    "            # Frobenius norm difference\n",
    "            features[\"spectrogram_frobenius_diff\"] = np.linalg.norm(Sxx_after_resized - Sxx_before_resized, 'fro')\n",
    "            \n",
    "            # Energy distribution differences\n",
    "            features[\"spectrogram_energy_before\"] = np.sum(Sxx_before_resized)\n",
    "            features[\"spectrogram_energy_after\"] = np.sum(Sxx_after_resized)\n",
    "            features[\"spectrogram_energy_ratio\"] = features[\"spectrogram_energy_after\"] / max(features[\"spectrogram_energy_before\"], 1e-10)\n",
    "        else:\n",
    "            features[\"spectrogram_mean_diff\"] = 0\n",
    "            features[\"spectrogram_frobenius_diff\"] = 0\n",
    "            features[\"spectrogram_energy_before\"] = 0\n",
    "            features[\"spectrogram_energy_after\"] = 0\n",
    "            features[\"spectrogram_energy_ratio\"] = 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Spectrogram features failed: {e}\")\n",
    "        features[\"spectrogram_mean_diff\"] = 0\n",
    "        features[\"spectrogram_frobenius_diff\"] = 0\n",
    "        features[\"spectrogram_energy_before\"] = 0\n",
    "        features[\"spectrogram_energy_after\"] = 0\n",
    "        features[\"spectrogram_energy_ratio\"] = 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_phase_spectrum_features(values_before, values_after):\n",
    "    \"\"\"Extract Phase Spectrum Discrepancy features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Compute FFT for both segments\n",
    "        fft_before = fft(values_before.values)\n",
    "        fft_after = fft(values_after.values)\n",
    "        \n",
    "        # Extract phase angles\n",
    "        phase_before = np.angle(fft_before)\n",
    "        phase_after = np.angle(fft_after)\n",
    "        \n",
    "        # Handle different lengths by taking minimum\n",
    "        min_len = min(len(phase_before), len(phase_after))\n",
    "        phase_before = phase_before[:min_len]\n",
    "        phase_after = phase_after[:min_len]\n",
    "        \n",
    "        # Phase differences\n",
    "        phase_diff = phase_after - phase_before\n",
    "        \n",
    "        # Unwrap phase differences to handle 2π discontinuities\n",
    "        phase_diff_unwrapped = np.unwrap(phase_diff)\n",
    "        \n",
    "        # Mean phase difference\n",
    "        features[\"phase_mean_diff\"] = np.mean(phase_diff_unwrapped)\n",
    "        \n",
    "        # Variance of phase differences\n",
    "        features[\"phase_variance_diff\"] = np.var(phase_diff_unwrapped)\n",
    "        \n",
    "        # Standard deviation of phase differences\n",
    "        features[\"phase_std_diff\"] = np.std(phase_diff_unwrapped)\n",
    "        \n",
    "        # Mean absolute phase difference\n",
    "        features[\"phase_mean_abs_diff\"] = np.mean(np.abs(phase_diff_unwrapped))\n",
    "        \n",
    "        # Phase coherence (circular correlation)\n",
    "        try:\n",
    "            phase_coherence = np.abs(np.mean(np.exp(1j * phase_diff)))\n",
    "            features[\"phase_coherence\"] = phase_coherence\n",
    "        except:\n",
    "            features[\"phase_coherence\"] = 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Phase spectrum features failed: {e}\")\n",
    "        features[\"phase_mean_diff\"] = 0\n",
    "        features[\"phase_variance_diff\"] = 0\n",
    "        features[\"phase_std_diff\"] = 0\n",
    "        features[\"phase_mean_abs_diff\"] = 0\n",
    "        features[\"phase_coherence\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_hilbert_envelope_features(values_before, values_after):\n",
    "    \"\"\"Extract Hilbert Envelope features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Compute analytic signals using Hilbert transform\n",
    "        analytic_before = signal.hilbert(values_before.values)\n",
    "        analytic_after = signal.hilbert(values_after.values)\n",
    "        \n",
    "        # Extract envelopes (amplitude)\n",
    "        envelope_before = np.abs(analytic_before)\n",
    "        envelope_after = np.abs(analytic_after)\n",
    "        \n",
    "        # Mean envelope values\n",
    "        features[\"envelope_mean_before\"] = np.mean(envelope_before)\n",
    "        features[\"envelope_mean_after\"] = np.mean(envelope_after)\n",
    "        features[\"envelope_mean_diff\"] = features[\"envelope_mean_after\"] - features[\"envelope_mean_before\"]\n",
    "        features[\"envelope_mean_ratio\"] = features[\"envelope_mean_after\"] / max(features[\"envelope_mean_before\"], 1e-10)\n",
    "        \n",
    "        # Envelope variance\n",
    "        features[\"envelope_var_before\"] = np.var(envelope_before)\n",
    "        features[\"envelope_var_after\"] = np.var(envelope_after)\n",
    "        features[\"envelope_var_diff\"] = features[\"envelope_var_after\"] - features[\"envelope_var_before\"]\n",
    "        features[\"envelope_var_ratio\"] = features[\"envelope_var_after\"] / max(features[\"envelope_var_before\"], 1e-10)\n",
    "        \n",
    "        # Mean absolute difference between envelopes\n",
    "        min_len = min(len(envelope_before), len(envelope_after))\n",
    "        envelope_before_trunc = envelope_before[:min_len]\n",
    "        envelope_after_trunc = envelope_after[:min_len]\n",
    "        \n",
    "        features[\"envelope_mean_abs_diff\"] = np.mean(np.abs(envelope_after_trunc - envelope_before_trunc))\n",
    "        \n",
    "        # Envelope shape similarity (correlation)\n",
    "        try:\n",
    "            correlation = np.corrcoef(envelope_before_trunc, envelope_after_trunc)[0, 1]\n",
    "            features[\"envelope_correlation\"] = correlation if not np.isnan(correlation) else 0\n",
    "        except:\n",
    "            features[\"envelope_correlation\"] = 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Hilbert envelope features failed: {e}\")\n",
    "        features[\"envelope_mean_before\"] = 0\n",
    "        features[\"envelope_mean_after\"] = 0\n",
    "        features[\"envelope_mean_diff\"] = 0\n",
    "        features[\"envelope_mean_ratio\"] = 1\n",
    "        features[\"envelope_var_before\"] = 0\n",
    "        features[\"envelope_var_after\"] = 0\n",
    "        features[\"envelope_var_diff\"] = 0\n",
    "        features[\"envelope_var_ratio\"] = 1\n",
    "        features[\"envelope_mean_abs_diff\"] = 0\n",
    "        features[\"envelope_correlation\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_instantaneous_frequency_features(values_before, values_after):\n",
    "    \"\"\"Extract Instantaneous Frequency features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Compute analytic signals\n",
    "        analytic_before = signal.hilbert(values_before.values)\n",
    "        analytic_after = signal.hilbert(values_after.values)\n",
    "        \n",
    "        # Compute instantaneous phase\n",
    "        inst_phase_before = np.unwrap(np.angle(analytic_before))\n",
    "        inst_phase_after = np.unwrap(np.angle(analytic_after))\n",
    "        \n",
    "        # Compute instantaneous frequency (derivative of phase)\n",
    "        inst_freq_before = np.diff(inst_phase_before) / (2 * np.pi)\n",
    "        inst_freq_after = np.diff(inst_phase_after) / (2 * np.pi)\n",
    "        \n",
    "        # Mean instantaneous frequency\n",
    "        features[\"inst_freq_mean_before\"] = np.mean(inst_freq_before)\n",
    "        features[\"inst_freq_mean_after\"] = np.mean(inst_freq_after)\n",
    "        features[\"inst_freq_mean_diff\"] = features[\"inst_freq_mean_after\"] - features[\"inst_freq_mean_before\"]\n",
    "        \n",
    "        # Instantaneous frequency variance\n",
    "        features[\"inst_freq_var_before\"] = np.var(inst_freq_before)\n",
    "        features[\"inst_freq_var_after\"] = np.var(inst_freq_after)\n",
    "        features[\"inst_freq_var_diff\"] = features[\"inst_freq_var_after\"] - features[\"inst_freq_var_before\"]\n",
    "        \n",
    "        # Instantaneous frequency range\n",
    "        features[\"inst_freq_range_before\"] = np.max(inst_freq_before) - np.min(inst_freq_before)\n",
    "        features[\"inst_freq_range_after\"] = np.max(inst_freq_after) - np.min(inst_freq_after)\n",
    "        features[\"inst_freq_range_diff\"] = features[\"inst_freq_range_after\"] - features[\"inst_freq_range_before\"]\n",
    "        \n",
    "        # Compare instantaneous frequency profiles\n",
    "        min_len = min(len(inst_freq_before), len(inst_freq_after))\n",
    "        if min_len > 0:\n",
    "            inst_freq_before_trunc = inst_freq_before[:min_len]\n",
    "            inst_freq_after_trunc = inst_freq_after[:min_len]\n",
    "            \n",
    "            features[\"inst_freq_profile_diff\"] = np.mean(np.abs(inst_freq_after_trunc - inst_freq_before_trunc))\n",
    "            \n",
    "            # Correlation between instantaneous frequency profiles\n",
    "            try:\n",
    "                correlation = np.corrcoef(inst_freq_before_trunc, inst_freq_after_trunc)[0, 1]\n",
    "                features[\"inst_freq_profile_correlation\"] = correlation if not np.isnan(correlation) else 0\n",
    "            except:\n",
    "                features[\"inst_freq_profile_correlation\"] = 0\n",
    "        else:\n",
    "            features[\"inst_freq_profile_diff\"] = 0\n",
    "            features[\"inst_freq_profile_correlation\"] = 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Instantaneous frequency features failed: {e}\")\n",
    "        features[\"inst_freq_mean_before\"] = 0\n",
    "        features[\"inst_freq_mean_after\"] = 0\n",
    "        features[\"inst_freq_mean_diff\"] = 0\n",
    "        features[\"inst_freq_var_before\"] = 0\n",
    "        features[\"inst_freq_var_after\"] = 0\n",
    "        features[\"inst_freq_var_diff\"] = 0\n",
    "        features[\"inst_freq_range_before\"] = 0\n",
    "        features[\"inst_freq_range_after\"] = 0\n",
    "        features[\"inst_freq_range_diff\"] = 0\n",
    "        features[\"inst_freq_profile_diff\"] = 0\n",
    "        features[\"inst_freq_profile_correlation\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_cepstral_coefficient_features(values_before, values_after):\n",
    "    \"\"\"Extract Cepstral Coefficient features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Number of cepstral coefficients to compute\n",
    "        n_coeffs = 10\n",
    "        \n",
    "        for segment_name, values in [('before', values_before), ('after', values_after)]:\n",
    "            # Compute FFT\n",
    "            fft_vals = fft(values.values)\n",
    "            \n",
    "            # Compute log magnitude spectrum\n",
    "            log_spectrum = np.log(np.abs(fft_vals) + 1e-10)\n",
    "            \n",
    "            # Compute cepstrum (IFFT of log spectrum)\n",
    "            cepstrum = np.real(ifft(log_spectrum))\n",
    "            \n",
    "            # Extract first n_coeffs cepstral coefficients\n",
    "            for i in range(min(n_coeffs, len(cepstrum))):\n",
    "                features[f\"{segment_name}_cepstral_c{i}\"] = cepstrum[i]\n",
    "        \n",
    "        # Compare cepstral coefficients\n",
    "        for i in range(n_coeffs):\n",
    "            before_key = f\"before_cepstral_c{i}\"\n",
    "            after_key = f\"after_cepstral_c{i}\"\n",
    "            \n",
    "            if before_key in features and after_key in features:\n",
    "                features[f\"cepstral_c{i}_diff\"] = features[after_key] - features[before_key]\n",
    "                features[f\"cepstral_c{i}_ratio\"] = features[after_key] / max(abs(features[before_key]), 1e-10)\n",
    "        \n",
    "        # Overall cepstral distance\n",
    "        before_coeffs = [features.get(f\"before_cepstral_c{i}\", 0) for i in range(n_coeffs)]\n",
    "        after_coeffs = [features.get(f\"after_cepstral_c{i}\", 0) for i in range(n_coeffs)]\n",
    "        \n",
    "        features[\"cepstral_distance\"] = np.linalg.norm(np.array(after_coeffs) - np.array(before_coeffs))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cepstral coefficient features failed: {e}\")\n",
    "        for i in range(10):\n",
    "            features[f\"before_cepstral_c{i}\"] = 0\n",
    "            features[f\"after_cepstral_c{i}\"] = 0\n",
    "            features[f\"cepstral_c{i}_diff\"] = 0\n",
    "            features[f\"cepstral_c{i}_ratio\"] = 1\n",
    "        features[\"cepstral_distance\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_resonant_frequency_features(values_before, values_after):\n",
    "    \"\"\"Extract Resonant Frequency (Multi-peak) features\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        n_peaks = 5  # Number of top peaks to analyze\n",
    "        \n",
    "        for segment_name, values in [('before', values_before), ('after', values_after)]:\n",
    "            # Compute FFT and power spectrum\n",
    "            fft_vals = fft(values.values)\n",
    "            power_spectrum = np.abs(fft_vals) ** 2\n",
    "            freqs = fftfreq(len(values), 1.0)\n",
    "            \n",
    "            # Only consider positive frequencies\n",
    "            pos_freqs = freqs[:len(freqs)//2]\n",
    "            pos_power = power_spectrum[:len(power_spectrum)//2]\n",
    "            \n",
    "            # Find peaks in the power spectrum\n",
    "            peaks, properties = signal.find_peaks(pos_power, height=0, distance=2)\n",
    "            \n",
    "            if len(peaks) > 0:\n",
    "                # Sort peaks by amplitude (descending)\n",
    "                peak_amplitudes = pos_power[peaks]\n",
    "                sorted_indices = np.argsort(peak_amplitudes)[::-1]\n",
    "                top_peaks = peaks[sorted_indices[:min(n_peaks, len(peaks))]]\n",
    "                top_amplitudes = peak_amplitudes[sorted_indices[:min(n_peaks, len(peaks))]]\n",
    "                \n",
    "                # Store peak frequencies and amplitudes\n",
    "                for i, (peak_idx, amplitude) in enumerate(zip(top_peaks, top_amplitudes)):\n",
    "                    features[f\"{segment_name}_peak_{i}_freq\"] = abs(pos_freqs[peak_idx])\n",
    "                    features[f\"{segment_name}_peak_{i}_amplitude\"] = amplitude\n",
    "            else:\n",
    "                # No peaks found\n",
    "                for i in range(n_peaks):\n",
    "                    features[f\"{segment_name}_peak_{i}_freq\"] = 0\n",
    "                    features[f\"{segment_name}_peak_{i}_amplitude\"] = 0\n",
    "        \n",
    "        # Compare peaks between segments\n",
    "        for i in range(n_peaks):\n",
    "            before_freq_key = f\"before_peak_{i}_freq\"\n",
    "            after_freq_key = f\"after_peak_{i}_freq\"\n",
    "            before_amp_key = f\"before_peak_{i}_amplitude\"\n",
    "            after_amp_key = f\"after_peak_{i}_amplitude\"\n",
    "            \n",
    "            if all(key in features for key in [before_freq_key, after_freq_key, before_amp_key, after_amp_key]):\n",
    "                # Frequency shifts\n",
    "                features[f\"peak_{i}_freq_shift\"] = abs(features[after_freq_key] - features[before_freq_key])\n",
    "                \n",
    "                # Amplitude changes\n",
    "                features[f\"peak_{i}_amplitude_diff\"] = features[after_amp_key] - features[before_amp_key]\n",
    "                features[f\"peak_{i}_amplitude_ratio\"] = features[after_amp_key] / max(features[before_amp_key], 1e-10)\n",
    "        \n",
    "        # Overall resonance shift metrics\n",
    "        before_freqs = [features.get(f\"before_peak_{i}_freq\", 0) for i in range(n_peaks)]\n",
    "        after_freqs = [features.get(f\"after_peak_{i}_freq\", 0) for i in range(n_peaks)]\n",
    "        \n",
    "        features[\"resonance_centroid_shift\"] = abs(np.mean(after_freqs) - np.mean(before_freqs))\n",
    "        features[\"resonance_spread_before\"] = np.std(before_freqs)\n",
    "        features[\"resonance_spread_after\"] = np.std(after_freqs)\n",
    "        features[\"resonance_spread_change\"] = features[\"resonance_spread_after\"] - features[\"resonance_spread_before\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Resonant frequency features failed: {e}\")\n",
    "        for i in range(5):\n",
    "            features[f\"before_peak_{i}_freq\"] = 0\n",
    "            features[f\"after_peak_{i}_freq\"] = 0\n",
    "            features[f\"before_peak_{i}_amplitude\"] = 0\n",
    "            features[f\"after_peak_{i}_amplitude\"] = 0\n",
    "            features[f\"peak_{i}_freq_shift\"] = 0\n",
    "            features[f\"peak_{i}_amplitude_diff\"] = 0\n",
    "            features[f\"peak_{i}_amplitude_ratio\"] = 1\n",
    "        features[\"resonance_centroid_shift\"] = 0\n",
    "        features[\"resonance_spread_before\"] = 0\n",
    "        features[\"resonance_spread_after\"] = 0\n",
    "        features[\"resonance_spread_change\"] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_all_features(df, include_dgp=True):\n",
    "    \"\"\"\n",
    "    Extract all features for structural break detection in time series data.\n",
    "    \n",
    "    This function analyzes a time series with 'value' and 'period' columns,\n",
    "    where period 0 represents the before segment and period 1 represents the after segment.\n",
    "    \n",
    "    Returns a comprehensive feature dictionary for structural break analysis.\n",
    "    \"\"\"\n",
    "    # Check if we have data in both periods\n",
    "    values_before = df[\"value\"][df[\"period\"] == 0]\n",
    "    values_after = df[\"value\"][df[\"period\"] == 1]\n",
    "\n",
    "    if values_before.empty or values_after.empty:\n",
    "        return {}  # Cannot extract features without data in both periods\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # ========================================\n",
    "    # 1. STATISTICAL TEST FEATURES\n",
    "    # Formal statistical tests specifically designed to detect structural breaks\n",
    "    # ========================================\n",
    "    test_features = extract_enhanced_statistical_features(df)\n",
    "    features.update(test_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 2. DESCRIPTIVE STATISTICS BY PERIOD\n",
    "    # Basic statistical measures calculated separately for before and after periods\n",
    "    # ========================================\n",
    "    before_stats = extract_optimized_descriptive_statistics(values_before)\n",
    "    after_stats = extract_optimized_descriptive_statistics(values_after)\n",
    "\n",
    "    # Add period-specific statistics with prefixes\n",
    "    for key, value in before_stats.items():\n",
    "        features[f\"before_{key}\"] = value\n",
    "\n",
    "    for key, value in after_stats.items():\n",
    "        features[f\"after_{key}\"] = value\n",
    "\n",
    "    # ========================================\n",
    "    # 3. TIME SERIES CHARACTERISTICS BY PERIOD\n",
    "    # Advanced time series properties like autocorrelation, trends, and seasonality patterns\n",
    "    # ========================================\n",
    "    before_ts = extract_optimized_time_series_features(values_before)\n",
    "    after_ts = extract_optimized_time_series_features(values_after)\n",
    "\n",
    "    for key, value in before_ts.items():\n",
    "        features[f\"before_{key}\"] = value\n",
    "    for key, value in after_ts.items():\n",
    "        features[f\"after_{key}\"] = value\n",
    "\n",
    "    # ========================================\n",
    "    # 4. DIFFERENCE AND CHANGE FEATURES\n",
    "    # Direct comparisons between periods to quantify the magnitude of structural changes\n",
    "    # ========================================\n",
    "    \n",
    "    # Statistical differences\n",
    "    diff_features = extract_difference_features(before_stats, after_stats)\n",
    "    features.update(diff_features)\n",
    "    \n",
    "    # Time series feature differences\n",
    "    ts_diff_features = extract_difference_features(before_ts, after_ts)\n",
    "    features.update(ts_diff_features)\n",
    "    \n",
    "    # Calculate percentage changes for key statistics\n",
    "    for key in before_stats:\n",
    "        diff = after_stats[key] - before_stats[key]\n",
    "        features[f\"{key}_diff\"] = diff\n",
    "\n",
    "        if abs(before_stats[key]) > 1e-10:\n",
    "            features[f\"{key}_pct_change\"] = diff / abs(before_stats[key])\n",
    "        else:\n",
    "            features[f\"{key}_pct_change\"] = 0\n",
    "\n",
    "    # ========================================\n",
    "    # 5. VOLATILITY AND RISK MEASURES\n",
    "    # Changes in variance, volatility, and risk characteristics between periods\n",
    "    # ========================================\n",
    "    before_returns = values_before.pct_change().dropna()\n",
    "    after_returns = values_after.pct_change().dropna()\n",
    "\n",
    "    if len(before_returns) > 0 and len(after_returns) > 0:\n",
    "        features[\"volatility_ratio\"] = after_returns.std() / max(before_returns.std(), 1e-10)\n",
    "    else:\n",
    "        features[\"volatility_ratio\"] = 1.0\n",
    "\n",
    "    # Value range comparison\n",
    "    features[\"value_range_ratio\"] = (values_after.max() - values_after.min()) / \\\n",
    "                                   max(values_before.max() - values_before.min(), 1e-10)\n",
    "\n",
    "    # ========================================\n",
    "    # 6. SEGMENT LENGTH AND STRUCTURAL PROPERTIES\n",
    "    # Basic properties of the time series segments and their relative characteristics\n",
    "    # ========================================\n",
    "    features[\"before_length\"] = len(values_before)\n",
    "    features[\"after_length\"] = len(values_after)\n",
    "    features[\"length_ratio\"] = len(values_after) / max(len(values_before), 1)\n",
    "\n",
    "    # ========================================\n",
    "    # 7. COMPRESSION-BASED COMPLEXITY FEATURES\n",
    "    # Information-theoretic measures that capture changes in data complexity and patterns\n",
    "    # ========================================\n",
    "    compression_features = extract_compression_features(values_before, values_after)\n",
    "    features.update(compression_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 8. COINTEGRATION AND RELATIONSHIP FEATURES\n",
    "    # Statistical measures of long-term relationships and equilibrium between periods\n",
    "    # ========================================\n",
    "    coint_features = extract_cointegration_features(values_before, values_after)\n",
    "    features.update(coint_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 9. CHOW TEST FEATURES\n",
    "    # Classical econometric test for structural breaks in regression relationships\n",
    "    # ========================================\n",
    "    chow_features = extract_chow_test_features(df)\n",
    "    features.update(chow_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 10. LIKELIHOOD RATIO TEST FEATURES\n",
    "    # Maximum likelihood-based tests for parameter stability and structural changes\n",
    "    # ========================================\n",
    "    lr_features = extract_likelihood_ratio_features(df)\n",
    "    features.update(lr_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 11. FOURIER SPECTRUM ANALYSIS FEATURES\n",
    "    # Frequency domain comparisons using Fast Fourier Transform to detect spectral shifts\n",
    "    # ========================================\n",
    "    fft_features = extract_fourier_spectrum_features(values_before, values_after)\n",
    "    features.update(fft_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 12. DOMINANT FREQUENCY SHIFT FEATURES\n",
    "    # Changes in primary oscillatory components and peak frequency locations\n",
    "    # ========================================\n",
    "    freq_shift_features = extract_dominant_frequency_features(values_before, values_after)\n",
    "    features.update(freq_shift_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 13. FREQUENCY BAND ENERGY FEATURES\n",
    "    # Distribution of spectral energy across different frequency ranges and their ratios\n",
    "    # ========================================\n",
    "    band_energy_features = extract_band_energy_features(values_before, values_after)\n",
    "    features.update(band_energy_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 14. WAVELET DECOMPOSITION FEATURES\n",
    "    # Multi-resolution analysis using wavelets to capture transient changes and scale-specific shifts\n",
    "    # ========================================\n",
    "    wavelet_features = extract_wavelet_energy_features(values_before, values_after)\n",
    "    features.update(wavelet_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 15. TIME-FREQUENCY SPECTROGRAM FEATURES\n",
    "    # Short-time Fourier transform analysis for time-localized frequency changes\n",
    "    # ========================================\n",
    "    spectrogram_features = extract_spectrogram_difference_features(values_before, values_after)\n",
    "    features.update(spectrogram_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 16. PHASE SPECTRUM ANALYSIS FEATURES\n",
    "    # Phase relationship changes and signal timing shifts between periods\n",
    "    # ========================================\n",
    "    phase_features = extract_phase_spectrum_features(values_before, values_after)\n",
    "    features.update(phase_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 17. HILBERT ENVELOPE FEATURES\n",
    "    # Analytic signal analysis for amplitude modulation and envelope shape changes\n",
    "    # ========================================\n",
    "    hilbert_features = extract_hilbert_envelope_features(values_before, values_after)\n",
    "    features.update(hilbert_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 18. INSTANTANEOUS FREQUENCY FEATURES\n",
    "    # Non-stationary frequency behavior and time-varying spectral characteristics\n",
    "    # ========================================\n",
    "    inst_freq_features = extract_instantaneous_frequency_features(values_before, values_after)\n",
    "    features.update(inst_freq_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 19. CEPSTRAL ANALYSIS FEATURES\n",
    "    # Cepstrum-based features for detecting periodicity, echoes, and filter characteristic changes\n",
    "    # ========================================\n",
    "    cepstral_features = extract_cepstral_coefficient_features(values_before, values_after)\n",
    "    features.update(cepstral_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 20. RESONANT FREQUENCY FEATURES\n",
    "    # Multi-peak spectral analysis for system resonance and modulation pattern detection\n",
    "    # ========================================\n",
    "    resonant_features = extract_resonant_frequency_features(values_before, values_after)\n",
    "    features.update(resonant_features)\n",
    "\n",
    "    # ========================================\n",
    "    # 21. DGP (DATA GENERATING PROCESS) FEATURES\n",
    "    # Features that characterize the underlying data generating process\n",
    "    # ========================================\n",
    "    if include_dgp:\n",
    "            try:\n",
    "                # Create a proper MultiIndex structure for DGP feature extraction\n",
    "                if not isinstance(df.index, pd.MultiIndex):\n",
    "                    # Create a temporary DataFrame with the proper structure\n",
    "                    df_copy = df.copy()\n",
    "                    \n",
    "                    # Add a dummy ID for the DGP extractor\n",
    "                    df_copy['id'] = 0\n",
    "                    \n",
    "                    # Use the existing index as time or create one\n",
    "                    if df_copy.index.name is None:\n",
    "                        df_copy = df_copy.reset_index(drop=True)\n",
    "                        df_copy['time'] = range(len(df_copy))\n",
    "                        time_col = 'time'\n",
    "                    else:\n",
    "                        time_col = df_copy.index.name\n",
    "                        df_copy = df_copy.reset_index()\n",
    "                    \n",
    "                    # Set MultiIndex\n",
    "                    df_copy = df_copy.set_index(['id', time_col])\n",
    "                    \n",
    "                    # Extract DGP features\n",
    "                    dgp_extractor = FastDGPFeatureExtractor()\n",
    "                    dgp_features = dgp_extractor.extract_features(df_copy, max_series=1)\n",
    "                    \n",
    "                    # Add DGP features to our feature dictionary\n",
    "                    if not dgp_features.empty:\n",
    "                        dgp_feature_dict = dgp_features.iloc[0].to_dict()\n",
    "                        dgp_feature_dict.pop('id', None)  # Remove id to avoid conflicts\n",
    "                        features.update(dgp_feature_dict)\n",
    "                else:\n",
    "                    # Already a MultiIndex, extract directly\n",
    "                    dgp_extractor = FastDGPFeatureExtractor()\n",
    "                    dgp_features = dgp_extractor.extract_features(df, max_series=1)\n",
    "                    \n",
    "                    if not dgp_features.empty:\n",
    "                        dgp_feature_dict = dgp_features.iloc[0].to_dict()\n",
    "                        dgp_feature_dict.pop('id', None)\n",
    "                        features.update(dgp_feature_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"DGP feature extraction failed: {e}\")\n",
    "                # Continue without DGP features\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_single_timeseries(args):\n",
    "    \"\"\"Helper function for parallel processing\"\"\"\n",
    "    ts_id, ts_data = args\n",
    "    try:\n",
    "        features = extract_all_features(ts_data)\n",
    "        if features:\n",
    "            features['id'] = ts_id\n",
    "            return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ts_id}: {e}\")\n",
    "    return None\n",
    "\n",
    "def create_feature_matrix(X, output_path=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Create feature matrix from time series DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        The input time series data with MultiIndex ['id', 'time']\n",
    "    output_path : str, optional\n",
    "        Path to save the extracted features. If None, features won't be saved.\n",
    "    verbose : bool, default=True\n",
    "        Whether to show progress bar\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing the extracted features\n",
    "    \"\"\"\n",
    "    # Ensure X is a DataFrame and has the proper multi-index structure\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "    # Check if the index is a MultiIndex\n",
    "    if not isinstance(X.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Input DataFrame must have a MultiIndex with levels ['id', 'time']\")\n",
    "\n",
    "    # Group by ID and extract features for each time series\n",
    "    ids = X.index.get_level_values('id').unique()\n",
    "    features_list = []\n",
    "\n",
    "    iterator = tqdm(ids) if verbose else ids\n",
    "    for ts_id in iterator:\n",
    "        # Get data for this time series\n",
    "        try:\n",
    "            ts_data = X.xs(ts_id, level='id')\n",
    "\n",
    "            # Extract all features\n",
    "            ts_features = extract_all_features(ts_data)\n",
    "\n",
    "            if ts_features:  # Skip if no features could be extracted\n",
    "                ts_features['id'] = ts_id\n",
    "                features_list.append(ts_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing time series ID {ts_id}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    feature_df = pd.DataFrame(features_list)\n",
    "\n",
    "    if feature_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Set ID as index\n",
    "    feature_df = feature_df.set_index('id')\n",
    "\n",
    "    # Handle missing values\n",
    "    feature_df = feature_df.fillna(0)\n",
    "\n",
    "    # Save features if output path is provided\n",
    "    if output_path is not None:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        print(f\"Saving extracted features to {output_path}\")\n",
    "\n",
    "        # Save features in different formats for flexibility\n",
    "        # CSV format (human-readable)\n",
    "        feature_df.to_csv(f\"{output_path}.csv\")\n",
    "\n",
    "        # Pickle format (preserves datatypes, faster to load)\n",
    "        feature_df.to_pickle(f\"{output_path}.pkl\")\n",
    "\n",
    "        # Parquet format (efficient storage, good for large datasets)\n",
    "        try:\n",
    "            feature_df.to_parquet(f\"{output_path}.parquet\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save parquet format. Error: {e}\")\n",
    "            print(\"Parquet format requires 'pyarrow' or 'fastparquet' package.\")\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(X_train: pd.DataFrame, y_train: pd.Series, model_directory_path: str, save_features=True):\n",
    "    \"\"\"\n",
    "    Train a model to detect structural breaks in time series\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        The input time series data with MultiIndex ['id', 'time']\n",
    "    y_train : pandas.Series\n",
    "        The target variable (1 for structural break, 0 for no break)\n",
    "    model_directory_path : str\n",
    "        Directory to save the trained model\n",
    "    save_features : bool, default=True\n",
    "        Whether to save the extracted features\n",
    "    \"\"\"\n",
    "    print(\"Training model for structural break detection\")\n",
    "    print(f\"X_train type: {type(X_train)}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_train index type: {type(X_train.index)}\")\n",
    "\n",
    "    # Ensure X_train has the right structure\n",
    "    if not isinstance(X_train.index, pd.MultiIndex):\n",
    "        print(\"Warning: X_train doesn't have a MultiIndex. Checking structure...\")\n",
    "        if 'id' in X_train.columns and 'time' in X_train.columns:\n",
    "            print(\"Converting to MultiIndex from columns.\")\n",
    "            X_train = X_train.set_index(['id', 'time'])\n",
    "        else:\n",
    "            print(\"ERROR: Cannot proceed without proper data structure\")\n",
    "            print(\"Expected MultiIndex with levels ['id', 'time'] or columns named 'id' and 'time'\")\n",
    "            # Save a dummy model that always predicts the majority class\n",
    "            majority_class = int(y_train.mean() >= 0.5) if len(y_train) > 0 else 0.5\n",
    "            dummy_model = {\"type\": \"dummy\", \"majority_class\": majority_class}\n",
    "            os.makedirs(os.path.dirname(model_directory_path), exist_ok=True)\n",
    "            joblib.dump(dummy_model, os.path.join(model_directory_path, 'model.joblib'))\n",
    "            return\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(model_directory_path, exist_ok=True)\n",
    "\n",
    "    # Define feature output path if saving features\n",
    "    feature_output_path = os.path.join(model_directory_path, 'train_features') if save_features else None\n",
    "\n",
    "    # print(\"Extracting features from training data...\")\n",
    "    # X_features = create_feature_matrix(X_train, output_path=feature_output_path)\n",
    "\n",
    "    # # Ensure we have features and they align with y_train\n",
    "    # if X_features.empty:\n",
    "    #     print(\"Warning: No features could be extracted from training data.\")\n",
    "    #     # Save a dummy model that always predicts the majority class\n",
    "    #     majority_class = int(y_train.mean() >= 0.5) if len(y_train) > 0 else 0.5\n",
    "    #     dummy_model = {\"type\": \"dummy\", \"majority_class\": majority_class}\n",
    "    #     joblib.dump(dummy_model, os.path.join(model_directory_path, 'model.joblib'))\n",
    "    #     return\n",
    "\n",
    "    X_features = pd.read_csv(os.path.join(model_directory_path, 'train_features.csv'), index_col='id')\n",
    "\n",
    "    # Align indices with y_train\n",
    "    common_indices = X_features.index.intersection(y_train.index)\n",
    "    X_features = X_features.loc[common_indices]\n",
    "    y_train = y_train.loc[common_indices]\n",
    "\n",
    "    # Save aligned target values\n",
    "    if save_features:\n",
    "        y_train.to_csv(os.path.join(model_directory_path, 'train_targets.csv'))\n",
    "\n",
    "    print(f\"Extracted {X_features.shape[1]} features for {X_features.shape[0]} time series\")\n",
    "\n",
    "    # Check if we have enough data to train\n",
    "    if len(X_features) < 10:\n",
    "        print(\"Warning: Not enough data to train a model.\")\n",
    "        # Save a dummy model that always predicts the majority class\n",
    "        majority_class = int(y_train.mean() >= 0.5)\n",
    "        dummy_model = {\"type\": \"dummy\", \"majority_class\": majority_class}\n",
    "        joblib.dump(dummy_model, os.path.join(model_directory_path, 'model.joblib'))\n",
    "        return\n",
    "    \n",
    "    # Calculate class weights for unbalanced data\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    import numpy as np\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    from sklearn.feature_selection import RFE\n",
    "\n",
    "    # Assuming you have y_train available\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    scale_pos_weight = class_weights[1] / class_weights[0] if len(class_weights) > 1 else 1\n",
    "\n",
    "    print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "    print(f\"Scale pos weight: {scale_pos_weight}\")\n",
    "\n",
    "    # Create a pipeline with preprocessing and model\n",
    "    pipeline = Pipeline([\n",
    "        \n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selector', RFE(\n",
    "        estimator=xgb.XGBClassifier(random_state=42),\n",
    "        n_features_to_select=500,  # Select top 500 features\n",
    "        step=15\n",
    "    )),\n",
    "        # ('model', xgb.XGBClassifier(\n",
    "        #         objective='binary:logistic',\n",
    "        #         n_estimators=997,  # Increase for better performance\n",
    "        #         max_depth=9,       # Reduce to prevent overfitting on minority class\n",
    "        #         learning_rate=0.013293258485719994,\n",
    "        #         subsample=0.942050971691917,\n",
    "        #         colsample_bylevel=0.8297860865775033,\n",
    "        #         colsample_bynode=0.8122445811896282,\n",
    "        #         #colsample_bytree=0.788,\n",
    "        #         random_state=89,\n",
    "        #         use_label_encoder=False,\n",
    "        #         eval_metric='logloss',\n",
    "                \n",
    "        #         # Key parameters for unbalanced classification:\n",
    "        #         #scale_pos_weight=0.4,  # Most important for imbalanced data\n",
    "        #         min_child_weight=10,                 # Higher values prevent overfitting\n",
    "        #         gamma= 3.837,                          # Minimum loss reduction for splits\n",
    "        #         reg_alpha=4.820456984721685,                      # L1 regularization\n",
    "        #         reg_lambda=0.1881481457256562,                     # L2 regularization\n",
    "        #         max_delta_step=2,                   # Helps with imbalanced datasets\n",
    "        # )) # 0.791\n",
    "\n",
    "        ('model', xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                n_estimators=2000,  # Increase for better performance\n",
    "                max_depth=8,       # Reduce to prevent overfitting on minority class\n",
    "                learning_rate=0.0117638346084812,\n",
    "                subsample=0.8880638162211006,\n",
    "                colsample_bylevel=0.8297860865775033,\n",
    "                colsample_bynode=0.8122445811896282,\n",
    "                colsample_bytree=0.6583959588880559,\n",
    "                random_state=89,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='auc',\n",
    "                \n",
    "                # Key parameters for unbalanced classification:\n",
    "                #scale_pos_weight=1.2002030800473853,  # Most important for imbalanced data\n",
    "                min_child_weight=15,                 # Higher values prevent overfitting\n",
    "                gamma= 0.9267499839984471,                          # Minimum loss reduction for splits\n",
    "                reg_alpha=0.37089061512357124,                      # L1 regularization\n",
    "                reg_lambda=0.0671185894939356,                     # L2 regularization\n",
    "                max_delta_step=2,                   # Helps with imbalanced datasets\n",
    "        )) # 0.803\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # objective='binary:logistic',\n",
    "            # n_estimators=550,  # Increase for better performance\n",
    "            # max_depth=15,       # Reduce to prevent overfitting on minority class\n",
    "            # learning_rate=0.016,\n",
    "            # subsample=0.8,\n",
    "            # colsample_bylevel=0.8,\n",
    "            # colsample_bynode=0.8,\n",
    "            # colsample_bytree=0.8,\n",
    "            # random_state=42,\n",
    "            # use_label_encoder=False,\n",
    "            # eval_metric='logloss',\n",
    "            \n",
    "            # # Key parameters for unbalanced classification:\n",
    "            # scale_pos_weight=scale_pos_weight,  # Most important for imbalanced data\n",
    "            # min_child_weight=5,                 # Higher values prevent overfitting\n",
    "            # gamma=0.35,                          # Minimum loss reduction for splits\n",
    "            # reg_alpha=0.1,                      # L1 regularization\n",
    "            # reg_lambda=1.0,                     # L2 regularization\n",
    "            # max_delta_step=1,                   # Helps with imbalanced datasets\n",
    "\n",
    "#     xgb_params = {\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'eval_metric': 'auc',\n",
    "#         'n_estimators': 2000,\n",
    "#         'learning_rate': 0.01,\n",
    "#         'max_depth': 7,  # Reduced from 15\n",
    "#         'subsample': 0.7,\n",
    "#         'colsample_bytree': 0.7,\n",
    "#         'gamma': 0.1,\n",
    "#         'reg_alpha': 0.1,\n",
    "#         'reg_lambda': 0.1,\n",
    "#         'n_jobs': -1,\n",
    "#         'random_state': 42,  # Changed from 'seed' to 'random_state'\n",
    "#         'scale_pos_weight': (len(y_train) - sum(y_train)) / sum(y_train)\n",
    "#     }\n",
    "        # ('model', xgb.XGBClassifier(\n",
    "        #     objective='binary:logistic',\n",
    "        #     n_estimators=2000,  # Increase for better performance\n",
    "        #     max_depth=7,       # Reduce to prevent overfitting on minority class\n",
    "        #     learning_rate=0.01,\n",
    "        #     subsample=0.7,\n",
    "        #     colsample_bytree= 0.7,\n",
    "        #     random_state=42,\n",
    "        #     use_label_encoder=False,\n",
    "        #     eval_metric='logloss',\n",
    "            \n",
    "        #     # Key parameters for unbalanced classification:\n",
    "        #     scale_pos_weight=scale_pos_weight,  # Most important for imbalanced data\n",
    "        #     gamma=0.1,                          # Minimum loss reduction for splits\n",
    "        #     reg_alpha= 0.1,                      # L1 regularization\n",
    "        #     reg_lambda=0.1,                     # L2 regularization\n",
    "        #     #max_delta_step=1,                   # Helps with imbalanced datasets\n",
    "        # ))\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    pipeline.fit(X_features, y_train)\n",
    "    # Get selected feature names after RFE\n",
    "    selected_mask = pipeline.named_steps['feature_selector'].get_support()\n",
    "    selected_feature_names = X_features.columns[selected_mask]\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importance = pipeline.named_steps['model'].feature_importances_\n",
    "\n",
    "    # Create a DataFrame of feature importances\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': selected_feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Display top 20 features\n",
    "    print(\"\\nTop 20 Most Important Features:\")\n",
    "    print(importance_df.head(20))\n",
    "\n",
    "    # Calculate training metrics\n",
    "    y_pred_proba = pipeline.predict_proba(X_features)[:, 1]\n",
    "    #y_pred = pipeline.predict(X_features)\n",
    "\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "    # accuracy = accuracy_score(y_train, y_pred_proba)\n",
    "    # f1 = f1_score(y_train, y_pred_proba)\n",
    "\n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Saving model to {os.path.join(model_directory_path, 'model.joblib')}\")\n",
    "    joblib.dump(pipeline, os.path.join(model_directory_path, 'model.joblib'))\n",
    "\n",
    "    # Save feature importance\n",
    "    importance_df.to_csv(os.path.join(model_directory_path, 'feature_importance.csv'), index=False)\n",
    "\n",
    "    # Save model metrics\n",
    "    metrics = {\n",
    "        'auc': auc,\n",
    "        # 'accuracy': accuracy,\n",
    "        # 'f1': f1,\n",
    "        'n_features': X_features.shape[1],\n",
    "        'n_samples': X_features.shape[0]\n",
    "    }\n",
    "\n",
    "    pd.Series(metrics).to_csv(os.path.join(model_directory_path, 'train_metrics.csv'))\n",
    "\n",
    "def predict(X_test: pd.DataFrame, model_directory_path: str, save_features=True) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Generate predictions for structural breaks in test data\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test : pandas.DataFrame\n",
    "        The input time series data with MultiIndex ['id', 'time']\n",
    "    model_directory_path : str\n",
    "        Directory containing the trained model\n",
    "    save_features : bool, default=True\n",
    "        Whether to save the extracted features\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        Predicted probabilities of structural break for each time series\n",
    "    \"\"\"\n",
    "    model_path = os.path.join(model_directory_path, 'model.joblib')\n",
    "\n",
    "    # Check if model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    # Load the model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # Debug information\n",
    "    print(f\"X_test type: {type(X_test)}\")\n",
    "    print(f\"X_test shape: {X_test}\")\n",
    "\n",
    "\n",
    "    if not isinstance(X_test.index, pd.MultiIndex):\n",
    "        print(\"Warning: X_test doesn't have a MultiIndex. Attempting to convert...\")\n",
    "        # Try to infer the structure or convert to expected format\n",
    "        if 'id' in X_test.columns and 'time' in X_test.columns:\n",
    "            # Set MultiIndex from columns\n",
    "            X_test = X_test.set_index(['id', 'time'])\n",
    "            print(\"Converted to MultiIndex from columns.\")\n",
    "        else:\n",
    "            print(\"Could not convert to proper MultiIndex structure.\")\n",
    "            # Get unique IDs if possible, or create a default response\n",
    "            if 'id' in X_test.columns:\n",
    "                ids = X_test['id'].unique()\n",
    "                return pd.Series(0.5, index=ids)\n",
    "            else:\n",
    "                # Create arbitrary IDs as a last resort\n",
    "                dummy_ids = np.arange(len(X_test) // 100 + 1)  # Rough estimate\n",
    "                return pd.Series(0.5, index=dummy_ids)\n",
    "\n",
    "    # Define feature output path if saving features\n",
    "    feature_output_path = os.path.join(model_directory_path, 'test_features') if save_features else None\n",
    "\n",
    "    # Check if we have a dummy model\n",
    "    if isinstance(model, dict) and model.get('type') == 'dummy':\n",
    "        # Return the majority class for all IDs\n",
    "        ids = X_test.index.get_level_values('id').unique()\n",
    "        return pd.Series(model['majority_class'], index=ids)\n",
    "\n",
    "    # Extract features from test data\n",
    "    print(\"Extracting features from test data...\")\n",
    "    X_features = create_feature_matrix(X_test, output_path=feature_output_path)\n",
    "\n",
    "    if X_features.empty:\n",
    "        print(\"Warning: No features could be extracted from test data.\")\n",
    "        # Return 0.5 for all IDs (neutral prediction)\n",
    "        ids = X_test.index.get_level_values('id').unique()\n",
    "        return pd.Series(0.5, index=ids)\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    probabilities = model.predict_proba(X_features)[:, 1]\n",
    "    #probabilities = model.predict(X_features)\n",
    "    # Create Series with predictions\n",
    "    predictions = pd.Series(probabilities, index=X_features.index)\n",
    "\n",
    "    # Save predictions if requested\n",
    "    if save_features:\n",
    "        predictions.to_csv(os.path.join(model_directory_path, 'test_predictions.csv'))\n",
    "    if save_features:\n",
    "        print(\"saved X_features_test\")\n",
    "        X_features.to_csv(os.path.join(\"C:\\\\Users\\Alon\\Desktop\\Breakpoint classifiction\\data\", 'X_features_test.csv'))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f4dce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import (\n",
    "    RFE, SelectKBest, mutual_info_classif, VarianceThreshold\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    StackingClassifier, RandomForestClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb\n",
    "\n",
    "# Optional: Try to import LightGBM\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM not available, using XGBoost and RandomForest only\")\n",
    "\n",
    "def train(X_train: pd.DataFrame, y_train: pd.Series, model_directory_path: str, save_features=True):\n",
    "    \"\"\"\n",
    "    Train a model to detect structural breaks in time series\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas.DataFrame\n",
    "        The input time series data with MultiIndex ['id', 'time']\n",
    "    y_train : pandas.Series\n",
    "        The target variable (1 for structural break, 0 for no break)\n",
    "    model_directory_path : str\n",
    "        Directory to save the trained model\n",
    "    save_features : bool, default=True\n",
    "        Whether to save the extracted features\n",
    "    \"\"\"\n",
    "    print(\"Training model for structural break detection\")\n",
    "    print(f\"X_train type: {type(X_train)}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_train index type: {type(X_train.index)}\")\n",
    "\n",
    "    # Ensure X_train has the right structure\n",
    "    if not isinstance(X_train.index, pd.MultiIndex):\n",
    "        print(\"Warning: X_train doesn't have a MultiIndex. Checking structure...\")\n",
    "        if 'id' in X_train.columns and 'time' in X_train.columns:\n",
    "            print(\"Converting to MultiIndex from columns.\")\n",
    "            X_train = X_train.set_index(['id', 'time'])\n",
    "        else:\n",
    "            print(\"ERROR: Cannot proceed without proper data structure\")\n",
    "            print(\"Expected MultiIndex with levels ['id', 'time'] or columns named 'id' and 'time'\")\n",
    "            # Save a dummy model that always predicts the majority class\n",
    "            majority_class = int(y_train.mean() >= 0.5) if len(y_train) > 0 else 0.5\n",
    "            dummy_model = {\"type\": \"dummy\", \"majority_class\": majority_class}\n",
    "            os.makedirs(os.path.dirname(model_directory_path), exist_ok=True)\n",
    "            joblib.dump(dummy_model, os.path.join(model_directory_path, 'model.joblib'))\n",
    "            return\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(model_directory_path, exist_ok=True)\n",
    "\n",
    "    # Load pre-extracted features\n",
    "    X_features = pd.read_csv(os.path.join(model_directory_path, 'train_features.csv'), index_col='id')\n",
    "\n",
    "    # Align indices with y_train\n",
    "    common_indices = X_features.index.intersection(y_train.index)\n",
    "    X_features = X_features.loc[common_indices]\n",
    "    y_train = y_train.loc[common_indices]\n",
    "\n",
    "    # Save aligned target values\n",
    "    if save_features:\n",
    "        y_train.to_csv(os.path.join(model_directory_path, 'train_targets.csv'))\n",
    "\n",
    "    print(f\"Extracted {X_features.shape[1]} features for {X_features.shape[0]} time series\")\n",
    "\n",
    "    # Check if we have enough data to train\n",
    "    if len(X_features) < 10:\n",
    "        print(\"Warning: Not enough data to train a model.\")\n",
    "        # Save a dummy model that always predicts the majority class\n",
    "        majority_class = int(y_train.mean() >= 0.5)\n",
    "        dummy_model = {\"type\": \"dummy\", \"majority_class\": majority_class}\n",
    "        joblib.dump(dummy_model, os.path.join(model_directory_path, 'model.joblib'))\n",
    "        return\n",
    "    \n",
    "    # Calculate class weights for unbalanced data\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    scale_pos_weight = class_weights[1] / class_weights[0] if len(class_weights) > 1 else 1\n",
    "\n",
    "    print(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "    print(f\"Scale pos weight: {scale_pos_weight}\")\n",
    "\n",
    "    def create_advanced_pipeline():\n",
    "        \"\"\"Create an ensemble pipeline with multiple models\"\"\"\n",
    "        \n",
    "        # Base models for ensemble\n",
    "        models = [\n",
    "            ('xgb_main', xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                n_estimators=1500,  # Reduced for ensemble\n",
    "                max_depth=8,\n",
    "                learning_rate=0.0117638346084812,\n",
    "                subsample=0.8880638162211006,\n",
    "                colsample_bylevel=0.8297860865775033,\n",
    "                colsample_bynode=0.8122445811896282,\n",
    "                colsample_bytree=0.6583959588880559,\n",
    "                random_state=89,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='auc',\n",
    "                min_child_weight=15,\n",
    "                gamma=0.9267499839984471,\n",
    "                reg_alpha=0.37089061512357124,\n",
    "                reg_lambda=0.0671185894939356,\n",
    "                max_delta_step=2,\n",
    "            )),\n",
    "            \n",
    "            ('model', xgb.XGBClassifier(\n",
    "                            objective='binary:logistic',\n",
    "                            n_estimators=2000,  # Increase for better performance\n",
    "                            max_depth=8,       # Reduce to prevent overfitting on minority class\n",
    "                            learning_rate=0.0117638346084812,\n",
    "                            subsample=0.8880638162211006,\n",
    "                            colsample_bylevel=0.8297860865775033,\n",
    "                            colsample_bynode=0.8122445811896282,\n",
    "                            colsample_bytree=0.6583959588880559,\n",
    "                            random_state=89,\n",
    "                            use_label_encoder=False,\n",
    "                            eval_metric='auc',\n",
    "                            \n",
    "                            # Key parameters for unbalanced classification:\n",
    "                            #scale_pos_weight=1.2002030800473853,  # Most important for imbalanced data\n",
    "                            min_child_weight=15,                 # Higher values prevent overfitting\n",
    "                            gamma= 0.9267499839984471,                          # Minimum loss reduction for splits\n",
    "                            reg_alpha=0.37089061512357124,                      # L1 regularization\n",
    "                            reg_lambda=0.0671185894939356,                     # L2 regularization\n",
    "                            max_delta_step=2,                   # Helps with imbalanced datasets\n",
    "                    )), # 0.803\n",
    "\n",
    "\n",
    "            ('xgb_alt', xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                n_estimators=1200,\n",
    "                max_depth=6,  # Different depth\n",
    "                learning_rate=0.02,  # Different learning rate\n",
    "                subsample=0.9,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,  # Different random state\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='auc',\n",
    "                min_child_weight=10,\n",
    "                gamma=0.5,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                scale_pos_weight=scale_pos_weight,\n",
    "            )),\n",
    "            \n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=800,\n",
    "                max_depth=10,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10,\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        # Add LightGBM if available\n",
    "        if LIGHTGBM_AVAILABLE:\n",
    "            models.append(\n",
    "                ('lgb', LGBMClassifier(\n",
    "                    objective='binary',\n",
    "                    n_estimators=1200,\n",
    "                    learning_rate=0.05,\n",
    "                    num_leaves=50,\n",
    "                    feature_fraction=0.8,\n",
    "                    bagging_fraction=0.8,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42,\n",
    "                    verbose=-1\n",
    "                ))\n",
    "            )\n",
    "        \n",
    "        # Choose ensemble method based on availability\n",
    "        if len(models) >= 3:  # Use stacking for 3+ models\n",
    "            print(f\"Creating stacking ensemble with {len(models)} models\")\n",
    "            ensemble = StackingClassifier(\n",
    "                estimators=models,\n",
    "                final_estimator=LogisticRegression(\n",
    "                    class_weight='balanced',\n",
    "                    max_iter=1000,\n",
    "                    random_state=42\n",
    "                ),\n",
    "                cv=3,  # Reduced for faster training\n",
    "                stack_method='predict_proba',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        else:  # Use voting for fewer models\n",
    "            print(f\"Creating voting ensemble with {len(models)} models\")\n",
    "            ensemble = VotingClassifier(\n",
    "                estimators=models,\n",
    "                voting='soft',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        \n",
    "        # Complete pipeline\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            \n",
    "            # Multi-stage feature selection\n",
    "            ('variance_filter', VarianceThreshold(0.01)),\n",
    "            ('univariate_selector', SelectKBest(\n",
    "                score_func=mutual_info_classif, \n",
    "                k=min(600, X_features.shape[1])  # Don't exceed available features\n",
    "            )),\n",
    "            ('rfe_selector', RFE(\n",
    "                estimator=xgb.XGBClassifier(\n",
    "                    n_estimators=100,  # Fast estimator for RFE\n",
    "                    random_state=42\n",
    "                ),\n",
    "                n_features_to_select=min(400, X_features.shape[1]//2),\n",
    "                step=20  # Larger step for faster selection\n",
    "            )),\n",
    "            \n",
    "            ('ensemble', ensemble)\n",
    "        ])\n",
    "\n",
    "    def create_simple_pipeline():\n",
    "        \"\"\"Fallback to simple single model pipeline\"\"\"\n",
    "        return Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('feature_selector', RFE(\n",
    "                estimator=xgb.XGBClassifier(random_state=42),\n",
    "                n_features_to_select=min(500, X_features.shape[1]),\n",
    "                step=15\n",
    "            )),\n",
    "            ('model', xgb.XGBClassifier(\n",
    "                objective='binary:logistic',\n",
    "                n_estimators=2000,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.0117638346084812,\n",
    "                subsample=0.8880638162211006,\n",
    "                colsample_bylevel=0.8297860865775033,\n",
    "                colsample_bynode=0.8122445811896282,\n",
    "                colsample_bytree=0.6583959588880559,\n",
    "                random_state=89,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric='auc',\n",
    "                min_child_weight=15,\n",
    "                gamma=0.9267499839984471,\n",
    "                reg_alpha=0.37089061512357124,\n",
    "                reg_lambda=0.0671185894939356,\n",
    "                max_delta_step=2,\n",
    "            ))\n",
    "        ])\n",
    "\n",
    "    # Try advanced pipeline, fall back to simple if needed\n",
    "    try:\n",
    "        print(\"Creating advanced ensemble pipeline...\")\n",
    "        pipeline = create_advanced_pipeline()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create advanced pipeline: {e}\")\n",
    "        print(\"Falling back to simple pipeline...\")\n",
    "        pipeline = create_simple_pipeline()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    pipeline.fit(X_features, y_train)\n",
    "    \n",
    "    # Get feature selection info\n",
    "    try:\n",
    "        if 'rfe_selector' in pipeline.named_steps:\n",
    "            selected_mask = pipeline.named_steps['rfe_selector'].get_support()\n",
    "            # Need to account for previous feature selection steps\n",
    "            if 'univariate_selector' in pipeline.named_steps:\n",
    "                # Get mask from univariate selector first\n",
    "                univariate_mask = pipeline.named_steps['univariate_selector'].get_support()\n",
    "                # Apply variance filter if it exists\n",
    "                if 'variance_filter' in pipeline.named_steps:\n",
    "                    variance_mask = pipeline.named_steps['variance_filter'].get_support()\n",
    "                    # Combine all masks\n",
    "                    final_mask = np.zeros(len(X_features.columns), dtype=bool)\n",
    "                    final_mask[variance_mask] = univariate_mask[selected_mask]\n",
    "                else:\n",
    "                    final_mask = np.zeros(len(X_features.columns), dtype=bool)\n",
    "                    final_mask[univariate_mask] = selected_mask\n",
    "                selected_feature_names = X_features.columns[final_mask]\n",
    "            else:\n",
    "                selected_feature_names = X_features.columns[selected_mask]\n",
    "        else:\n",
    "            # Fallback for simple pipeline\n",
    "            selected_mask = pipeline.named_steps['feature_selector'].get_support()\n",
    "            selected_feature_names = X_features.columns[selected_mask]\n",
    "\n",
    "        print(f\"Selected {len(selected_feature_names)} features\")\n",
    "        \n",
    "        # Get feature importances (works for ensemble too - uses the final estimator)\n",
    "        if hasattr(pipeline.named_steps['ensemble' if 'ensemble' in pipeline.named_steps else 'model'], 'feature_importances_'):\n",
    "            if 'ensemble' in pipeline.named_steps and hasattr(pipeline.named_steps['ensemble'], 'final_estimator_'):\n",
    "                # For stacking classifier, get importances from meta-learner\n",
    "                if hasattr(pipeline.named_steps['ensemble'].final_estimator_, 'coef_'):\n",
    "                    feature_importance = np.abs(pipeline.named_steps['ensemble'].final_estimator_.coef_[0])\n",
    "                else:\n",
    "                    feature_importance = np.ones(len(selected_feature_names))  # Fallback\n",
    "            else:\n",
    "                # For single model or voting classifier\n",
    "                feature_importance = pipeline.named_steps['ensemble' if 'ensemble' in pipeline.named_steps else 'model'].feature_importances_\n",
    "            \n",
    "            # Create a DataFrame of feature importances\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': selected_feature_names,\n",
    "                'Importance': feature_importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "\n",
    "            # Display top 20 features\n",
    "            print(\"\\nTop 20 Most Important Features:\")\n",
    "            print(importance_df.head(20))\n",
    "            \n",
    "            # Save feature importance\n",
    "            importance_df.to_csv(os.path.join(model_directory_path, 'feature_importance.csv'), index=False)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract feature importance: {e}\")\n",
    "\n",
    "    # Calculate training metrics\n",
    "    y_pred_proba = pipeline.predict_proba(X_features)[:, 1]\n",
    "    auc = roc_auc_score(y_train, y_pred_proba)\n",
    "\n",
    "    print(f\"\\nTraining Metrics:\")\n",
    "    print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    print(f\"Saving model to {os.path.join(model_directory_path, 'model.joblib')}\")\n",
    "    joblib.dump(pipeline, os.path.join(model_directory_path, 'model.joblib'))\n",
    "\n",
    "    # Save model metrics\n",
    "    metrics = {\n",
    "        'auc': auc,\n",
    "        'n_features': X_features.shape[1],\n",
    "        'n_samples': X_features.shape[0],\n",
    "        'model_type': 'ensemble' if 'ensemble' in pipeline.named_steps else 'single'\n",
    "    }\n",
    "\n",
    "    pd.Series(metrics).to_csv(os.path.join(model_directory_path, 'train_metrics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb9410",
   "metadata": {},
   "source": [
    "### The `infer()` Function\n",
    "\n",
    "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
    "\n",
    "**Important workflow:**\n",
    "1. Load your model;\n",
    "2. Use the `yield` statement to signal readiness to the runner;\n",
    "3. Process each dataset one by one within the for loop;\n",
    "4. For each dataset, use `yield prediction` to return your prediction.\n",
    "\n",
    "**Note:** The datasets can only be iterated once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea124be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "def infer(\n",
    "    X_test: Iterable[pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    Generator function that yields predictions for structural breaks in streaming data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test : Iterable[pandas.DataFrame]\n",
    "        An iterable of DataFrames, each containing time series data with 'value' and 'period' columns\n",
    "    model_directory_path : str\n",
    "        Directory containing the trained model\n",
    "\n",
    "    Yields:\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Predicted probabilities of structural break for each time series\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model_path = os.path.join(model_directory_path, 'model.joblib')\n",
    "\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "    # Signal that we're ready to process data\n",
    "    yield\n",
    "\n",
    "    # Process each incoming dataset\n",
    "    for dataset in X_test:\n",
    "        try:\n",
    "            # Check if we have a dummy model\n",
    "            if isinstance(model, dict) and model.get('type') == 'dummy':\n",
    "                # For dummy model, just return the majority class\n",
    "                prediction = pd.Series(model['majority_class'])\n",
    "                yield prediction\n",
    "                continue\n",
    "\n",
    "            # Extract features from the dataset\n",
    "            features = extract_all_features(dataset)\n",
    "\n",
    "            # Convert features to DataFrame for model input\n",
    "            if features:\n",
    "                features_df = pd.DataFrame([features])\n",
    "\n",
    "                # Ensure we have the same features as the model expects\n",
    "                if hasattr(model, 'feature_names_in_'):\n",
    "                    # For scikit-learn pipelines with feature names\n",
    "                    missing_cols = set(model.feature_names_in_) - set(features_df.columns)\n",
    "                    for col in missing_cols:\n",
    "                        features_df[col] = 0  # Fill missing features with zeros\n",
    "\n",
    "                    # Ensure correct column order\n",
    "                    features_df = features_df[model.feature_names_in_]\n",
    "\n",
    "                # Generate prediction\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    # For classifiers that output probabilities\n",
    "                    prob = model.predict_proba(features_df)[:, 1]\n",
    "                    prediction = pd.Series(prob)\n",
    "                else:\n",
    "                    # For models that only output class labels\n",
    "                    pred = model.predict(features_df)\n",
    "                    prediction = pd.Series(pred)\n",
    "            else:\n",
    "                # If no features could be extracted, return neutral prediction\n",
    "                prediction = pd.Series(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs, log it and return neutral prediction\n",
    "            print(f\"Error in prediction: {str(e)}\")\n",
    "            prediction = pd.Series(0.5)\n",
    "\n",
    "        # Yield the prediction\n",
    "        yield prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0b3dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    X_test: Iterable[pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    Generator function that yields predictions for structural breaks in streaming data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_test : Iterable[pandas.DataFrame]\n",
    "        An iterable of DataFrames, each containing time series data with 'value' and 'period' columns\n",
    "    model_directory_path : str\n",
    "        Directory containing the trained model\n",
    "\n",
    "    Yields:\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Predicted probabilities of structural break for each time series\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model_path = os.path.join(model_directory_path, 'model.joblib')\n",
    "\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "    # Signal that we're ready to process data\n",
    "    yield\n",
    "\n",
    "    # Initialize DGP extractor\n",
    "    dgp_extractor = FastDGPFeatureExtractor()\n",
    "    \n",
    "    # Process each incoming dataset\n",
    "    for dataset in X_test:\n",
    "        try:\n",
    "            # Check if we have a dummy model\n",
    "            if isinstance(model, dict) and model.get('type') == 'dummy':\n",
    "                # For dummy model, just return the majority class\n",
    "                prediction = pd.Series(model['majority_class'])\n",
    "                yield prediction\n",
    "                continue\n",
    "\n",
    "            # Extract features from the dataset\n",
    "            features = extract_all_features(dataset)\n",
    "            \n",
    "            # Extract DGP features specifically for this dataset\n",
    "            try:\n",
    "                # Create a proper MultiIndex structure for DGP feature extraction\n",
    "                # First, ensure we have a time index\n",
    "                if not isinstance(dataset.index, pd.DatetimeIndex) and 'time' not in dataset.columns:\n",
    "                    # Create a simple integer index if no time index exists\n",
    "                    dataset_with_index = dataset.reset_index(drop=True)\n",
    "                    dataset_with_index['time'] = range(len(dataset))\n",
    "                else:\n",
    "                    dataset_with_index = dataset.copy()\n",
    "                \n",
    "                # Create MultiIndex with a dummy ID for this single time series\n",
    "                if 'time' in dataset_with_index.columns:\n",
    "                    time_col = 'time'\n",
    "                else:\n",
    "                    time_col = dataset_with_index.index.name if dataset_with_index.index.name else 'time'\n",
    "                    dataset_with_index = dataset_with_index.reset_index()\n",
    "                \n",
    "                # Create MultiIndex\n",
    "                dataset_with_index['id'] = 0  # Dummy ID for single series\n",
    "                dataset_with_index = dataset_with_index.set_index(['id', time_col])\n",
    "                \n",
    "                # Extract DGP features\n",
    "                dgp_features = dgp_extractor.extract_features(dataset_with_index, max_series=1)\n",
    "                \n",
    "                # Add DGP features to our feature dictionary\n",
    "                if not dgp_features.empty:\n",
    "                    dgp_feature_dict = dgp_features.iloc[0].to_dict()\n",
    "                    # Remove the 'id' key if it exists to avoid conflicts\n",
    "                    dgp_feature_dict.pop('id', None)\n",
    "                    features.update(dgp_feature_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"DGP feature extraction failed in infer: {e}\")\n",
    "                # Continue without DGP features\n",
    "\n",
    "            # Convert features to DataFrame for model input\n",
    "            if features:\n",
    "                features_df = pd.DataFrame([features])\n",
    "\n",
    "                # Ensure we have the same features as the model expects\n",
    "                if hasattr(model, 'feature_names_in_'):\n",
    "                    # For scikit-learn pipelines with feature names\n",
    "                    missing_cols = set(model.feature_names_in_) - set(features_df.columns)\n",
    "                    for col in missing_cols:\n",
    "                        features_df[col] = 0  # Fill missing features with zeros\n",
    "\n",
    "                    # Ensure correct column order\n",
    "                    features_df = features_df[model.feature_names_in_]\n",
    "\n",
    "                # Generate prediction\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    # For classifiers that output probabilities\n",
    "                    prob = model.predict_proba(features_df)[:, 1]\n",
    "                    prediction = pd.Series(prob)\n",
    "                else:\n",
    "                    # For models that only output class labels\n",
    "                    pred = model.predict(features_df)\n",
    "                    prediction = pd.Series(pred)\n",
    "            else:\n",
    "                # If no features could be extracted, return neutral prediction\n",
    "                prediction = pd.Series(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs, log it and return neutral prediction\n",
    "            print(f\"Error in prediction: {str(e)}\")\n",
    "            prediction = pd.Series(0.5)\n",
    "\n",
    "        # Yield the prediction\n",
    "        yield prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24948dc0",
   "metadata": {},
   "source": [
    "## Local testing\n",
    "\n",
    "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
    "Even if it is not perfect, it should give you a quick idea if your model is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d193adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:14:35\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m16:14:35\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m16:14:35\u001b[0m started\n",
      "\u001b[32m16:14:35\u001b[0m running local test\n",
      "\u001b[32m16:14:35\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m16:14:35\u001b[0m \n",
      "\u001b[32m16:14:36\u001b[0m starting unstructured loop...\n",
      "\u001b[32m16:14:36\u001b[0m executing - command=train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
      "data\\X_train.parquet: already exists, file length match\n",
      "data\\X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
      "data\\X_test.reduced.parquet: already exists, file length match\n",
      "data\\y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
      "data\\y_train.parquet: already exists, file length match\n",
      "data\\y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
      "data\\y_test.reduced.parquet: already exists, file length match\n",
      "Training model for structural break detection\n",
      "X_train type: <class 'pandas.core.frame.DataFrame'>\n",
      "X_train shape: (23715734, 2)\n",
      "X_train index type: <class 'pandas.core.indexes.multi.MultiIndex'>\n",
      "Extracted 490 features for 10001 time series\n",
      "Class distribution: [7092 2909]\n",
      "Scale pos weight: 2.4379511859745615\n",
      "Creating advanced ensemble pipeline...\n",
      "Creating stacking ensemble with 5 models\n",
      "Training model...\n",
      "Could not extract feature importance: NumPy boolean array indexing assignment cannot assign 245 input values to the 489 output values where the mask is true\n",
      "\n",
      "Training Metrics:\n",
      "AUC-ROC: 1.0000\n",
      "Saving model to resources\\model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:17:54\u001b[0m executing - command=infer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from resources\\model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:18:15\u001b[0m checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
      "\u001b[32m16:18:15\u001b[0m executing - command=infer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from resources\\model.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m16:18:22\u001b[0m determinism check: passed\n",
      "\u001b[32m16:18:22\u001b[0m \u001b[33msave prediction - path=data\\prediction.parquet\u001b[0m\n",
      "\u001b[32m16:18:22\u001b[0m ended\n",
      "\u001b[32m16:18:22\u001b[0m \u001b[33mduration - time=00:03:46\u001b[0m\n",
      "\u001b[32m16:18:22\u001b[0m \u001b[33mmemory - before=\"6.64 GB\" after=\"3.93 GB\" consumed=\"-2709954560 bytes\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "crunch.test(\n",
    "    # Uncomment to disable the train\n",
    "    #force_first_train=False,\n",
    "\n",
    "    # Uncomment to disable the determinism check\n",
    "    # no_determinism_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a21aa",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a0bafee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.655638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.307177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.447560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.315210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.497388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>0.416451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>0.268131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>0.525434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>0.325190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.576498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction\n",
       "id               \n",
       "10001    0.655638\n",
       "10002    0.307177\n",
       "10003    0.447560\n",
       "10004    0.315210\n",
       "10005    0.497388\n",
       "...           ...\n",
       "10097    0.416451\n",
       "10098    0.268131\n",
       "10099    0.525434\n",
       "10100    0.325190\n",
       "10101    0.576498\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289889c",
   "metadata": {},
   "source": [
    "### Local scoring\n",
    "\n",
    "You can call the function that the system uses to estimate your score locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b581ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8117370892018779"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the targets\n",
    "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
    "\n",
    "# Call the scoring function\n",
    "sklearn.metrics.roc_auc_score(\n",
    "    target,\n",
    "    prediction,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
